{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeoBTAA Metadata Handbook","text":"<p>This handbook describes how to harvest and format metadata records for the BTAA Geoportal.</p>"},{"location":"#reference","title":"Reference","text":"<p>Information about the GeoBTAA Metadata Application Profile and our harvest guidelines.</p>"},{"location":"#explanation","title":"Explanation","text":"<p>Descriptions and clarifications of processes, content organization, policies, and tools</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Short, easy to complete exercises to help someone get the basics of running and writing scripts to harvest metadata.</p>"},{"location":"#recipes","title":"Recipes","text":"<p>Multi-step workflows for harvesting and editing metadata for the BTAA Geoportal.</p>"},{"location":"#who-is-this-handbook-for","title":"Who is this handbook for?","text":"<ul> <li> <p>Team Members in the Big Ten Academic Alliance Geospatial Information Network (BTAA-GIN)</p> </li> <li> <p>Development &amp; Operations Staff in the BTAA-GIN</p> </li> <li> <p>Users &amp; developers of open-source geospatial projects, such as OpenGeoMetadata and GeoBlacklight</p> </li> <li> <p>Contributors to the  BTAA Geoportal</p> </li> <li> <p>Users of the BTAA Geoportal</p> </li> </ul> Metadata Handbook Version History <p>Changes for Version 5.2 (February 4, 2025)</p> <p>This release includes documentation for several new metadata fields and updated worflows.</p> <ul> <li>Five new metadata fields on the Custom Elements page</li> <li>An updated workflow for the ArcGIS Hub recipe</li> <li>Description of a new tutorial about changing paths in the Terminal.</li> <li>Removing links to documentation about GEOMG, now replaced by GBL Admin</li> <li>An updated and refined the Resource Lifecycle steps and accompanying graphics.</li> </ul> <p>Changes for Version 5.1 (September 25, 2023)</p> <p>This version adds several new recipes and page cleanups.</p> <ul> <li>New recipes for: <ul> <li>cleaning metadata</li> <li>adding bounding boxes</li> <li>normalizing creators</li> <li>updating our list of ArcGIS Hubs</li> <li>how to add multiple download links in GEOMG</li> </ul> </li> <li>Updates the documentation for the ArcGIS, Socrata, and PASDA recipes. </li> <li>Updates the DCAT and CKAN documentation pages.</li> </ul> <p>Changes for Version 5.0 (May 24, 2023)</p> <p>This version incorporates the Harvesting Guide notebooks and includes documentation for harvesting metadata from different sources.</p> <ul> <li>New page describing the Tutorials in the Harvesting Guide</li> <li>Eight Recipe pages corresponding to Harvesting Guide</li> <li>Updated header design to match Geoportal      </li> </ul> <p>Changes for Version 4.6 (March 15, 2023)</p> <ul> <li>New page for manually adding bounding boxes</li> <li>Restructure using Diataxis framework</li> <li>Remove some GEOMG how to guidelines (moved to GEOMG Wiki)</li> <li>Clarify Editing Template differences from OGM-Aaardvark documentation</li> <li>Added Collection Development Policy and Curation Priorities documents</li> <li>Update input guidelines for Spatial Coverage (FAST IDs)</li> </ul> <p>Changes for Version 4.5.1 (February 28, 2023)</p> <ul> <li>Update version documentation</li> <li>Add link to generated PDF</li> </ul> <p>Changes for Version 4.5 (February 28, 2023)</p> <ul> <li>Add Creator ID</li> <li>Update input guidelines for Creator, Creator ID</li> <li>Remove Harvesting Guide info (migrating to separate site)</li> <li>Edit Submitting Metadata page</li> <li>Minor copy editing</li> <li>Add PDF export capability</li> </ul> <p>Changes for Version 4.4 (August 23, 2022)</p> <ul> <li>updated theme</li> <li>reorganized and expanded navigation menu</li> <li>new sections for Harvesting Guide and using GEOMG</li> </ul> <p>Changes for Version 4.3 (August 15, 2022)</p> <ul> <li>migrate to MkDocs.org platform</li> <li>update bounding box entry guidelines</li> <li>add GEOMG page</li> </ul> <p>Changes for Version 4.2 (March 24, 2022)</p> <ul> <li>New Entry and Usage Guidelines page</li> <li>Expands content organization model documentation</li> <li>Changes the name of the schema from 'Aardvark' to 'OpenGeoMetadata (version Aardvark)'</li> <li>Cleans up outdated links</li> </ul> <p>Changes for Version 4.1 (Jan 2022)</p> <ul> <li>updates Status as optional; removes controlled vocabulary</li> <li>Clarifies relationship model</li> </ul> <p>Changes for Version 4.0 (July 2021)</p> <ul> <li>Incorporation of GEOMG Metadata Editor</li> <li>Upgrade to Aardvark Metadata Schema for GeoBlacklight</li> </ul> <p>Changes for version 3.3 (May 13, 2020)</p> <ul> <li>Added University of Nebraska</li> <li>Reorganized Metadata Elements to match editing template</li> <li>Updated the \u201cUpdate the Collections\u201d section to match new administrative process for tracking records</li> </ul> <p>Changes for version 3.2 (Jan 8, 2020)</p> <ul> <li>Added Date Range element</li> </ul> <p>Changes for version 3.1 (Dec 19, 2019)</p> <ul> <li>Added collection level records metadata schema</li> </ul> <p>Changes for version 3 (Oct 2019)</p> <ul> <li>GeoNetwork and Omeka deprecated</li> <li>all GeoBlacklight records are stored in a spreadsheet in Google Sheets</li> <li>records are transformed from CSV to GeoBlacklight JSON with a Python script</li> <li>additional metadata fields were added for administrative purposes</li> <li>IsPartOf field now holds a code pointing to the collection record</li> <li>Administrative groupings such as \u201cState agencies geospatial data\u201d are now subjects, not a Collection</li> <li>updated editing templates available</li> <li>all supplemental metadata can be stored as XML or HTML in project hosted folder</li> <li>updated links to collections database </li> </ul>"},{"location":"GEOMG/","title":"About GEOMG","text":"What is it? <p>GEOMG is a custom tool that functions as a backend metadata editor and manager for the GeoBlacklight application. </p> Who uses it? <p>BTAA-GIN Operations technical staff at the University of Minnesota</p> Who developed it? <p>The BTAA Geoportal Lead Developer, Eric Larson, created GEOMG, with direction from the BTAA-GIN. It is based upon the Kithe framework.</p> <p>Can other GeoBlacklight projects adopt it?</p> <p>We are currently working on offering this tool as a plugin for GeoBlacklight.</p> <p>In the meantime, this presentation describes the motivation for building the tool and a few screencasts showing how it works:</p>"},{"location":"about-harvesting/","title":"About harvesting","text":"<p>This page describes some of the processes and terminology associated with extracting metadata from various sources.</p>"},{"location":"about-harvesting/#what-is-web-scraping","title":"What is web scraping?","text":"<p>Web scraping is the process of programmatically collecting and extracting information from websites using automated scripts or bots. Common web scraping tools include pandas, Beautiful Soup, and WGET.</p>"},{"location":"about-harvesting/#what-is-data-harvesting","title":"What is data harvesting?","text":"<p>Data harvesting refers to the process of collecting large volumes of data from various sources, such as websites, social media, or other online platforms. This can involve using automated scripts or tools to extract structured or unstructured data, such as text, images, videos, or other types of content. The collected data can be used for various purposes, such as data analysis or content aggregation.</p>"},{"location":"about-harvesting/#what-is-metadata-harvesting","title":"What is metadata harvesting?","text":"<p>Metadata harvesting refers specifically to the process of collecting metadata from digital resources, such as websites, online databases, or digital libraries. Metadata is information that describes other data, such as the title, author, date, format, or subject of a document. Metadata harvesting involves extracting this information from digital resources and storing it in a structured format, such as a database or a metadata record.</p> <p>Metadata harvesting is often used in the context of digital libraries, archives, or repositories, where the metadata is used to organize and manage large collections of digital resources. Metadata harvesting can be done using specialized tools or protocols, such as the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH), which is a widely used standard for sharing metadata among digital repositories.</p>"},{"location":"about-harvesting/#do-scraping-and-harvesting-mean-the-same-thing","title":"Do \"scraping\" and \"harvesting\" mean the same thing?","text":"<p>The terms \"harvesting\" and \"scraping\" are often used interchangeably. However, there may be subtle differences in the way these terms are used, depending on the context.</p> <p>In general, scraping refers to the process of programmatically extracting data from websites using automated scripts or bots. The term \"scraping\" often implies a more aggressive approach, where data is extracted without explicit permission from the website owner. Scraping may involve parsing HTML pages, following links, and using techniques such as web crawling or screen scraping to extract data from websites.</p> <p>On the other hand, harvesting may refer to a more structured and systematic approach to extracting data from websites. The term \"harvesting\" often implies a more collaborative approach, where data is extracted with the explicit permission of the website owner or through APIs or web services provided by the website. Harvesting may involve using specialized software or tools to extract metadata, documents, or other resources from websites.</p>"},{"location":"about-harvesting/#what-is-web-parsing","title":"What is web parsing?","text":"<p>Web parsing refers to the process of scanning structured documents and extracting information. Although, it usually refers to parsing HTML pages, it can also describe parsing XML or JSON documents. Tools designed for this purpose, such as Beautiful Soup, are often called \"parsers.\"</p>"},{"location":"about-harvesting/#what-is-extract-transform-load-etl","title":"What is Extract-Transform-Load (ETL)?","text":"<p>ETL (Extract Transform Load) is a process of extracting data from one or more sources, transforming it to fit the target system's data model, and then loading it into the target system, such as a database or a data warehouse.</p> <p>The ETL process typically involves three main stages:</p> <ul> <li>Extract: This stage involves retrieving data from one or more sources, which may include databases, files, APIs, web services, or other data sources.</li> <li>Transform: This stage involves converting, cleaning, and restructuring the extracted data to fit the target system's data model and business rules. This may include tasks such as data mapping, data validation, data enrichment, data aggregation, or data cleansing.</li> <li>Load: This stage involves inserting or updating the transformed data into the target system, such as a database or a data warehouse. This may involve tasks such as data partitioning, indexing, or data quality checks.</li> </ul>"},{"location":"annual-review/","title":"Annual Review of Catalog Websites","text":""},{"location":"annual-review/#background-and-purpose","title":"Background and Purpose","text":"<p>We do annual reviews of websites with a lot of resources that can't easily be updated or evaluated by an existing script or recipe, usually due to issues with the APIs or site structure. </p> <p>The purpose of an annual review is not to fix or update all records associated with a website, but simply to assess whether the website has changed, evaluate the magnitude of any changes, and make a recommendation for further action. If many resources have been added or removed from the website, if the site structure has changed significantly, or if many of our associated records in GBL have broken links, then the site will need to be partially or completely re-harvested. </p>"},{"location":"annual-review/#how-to-complete-an-annual-review","title":"How to Complete an Annual Review","text":"<p>Use the GitHub issue to track your progress and record your results. This process is best done with a wide screen or multiple monitors so you can view two windows next to each other. </p>"},{"location":"annual-review/#evaluate-the-main-website-record-page","title":"Evaluate the main website record page","text":"<ol> <li> <p>Visit the Geoportal resource page for the parent website that you're reviewing. You can typically find the link associated with the GitHub issue, or you can search the Geoportal directly, usually under the Websites Resource Class. </p> </li> <li> <p>Use the blue Visit Source link in the right sidebar to open the source site in a new tab or window. You will compare this site to the records in the Geoportal, so viewing them side by side is helpful. </p> </li> <li> <p>If the Visit Source link is broken or doesn't bring you to the expected source website, try to locate the correct URL for this website. Use search engines and/or look for mentions of GIS or maps on the site(s) of the agency or entity providing the records. Make a note in the GitHub issue on the status of the Visit Source link and any new URLs, as appropriate. </p> </li> <li> <p>Check the Title, Description, and Format sections of the main website record page in the Geoportal. Is this information still accurate? Note any changes in the GitHub issue.</p> </li> </ol>"},{"location":"annual-review/#compare-the-geoportal-records-with-the-resources-on-the-source-website","title":"Compare the Geoportal records with the resources on the source website","text":"<ol> <li> <p>On the Geoportal website record, click Browse all [number] records under the  Has part... section. This will open a search for Is Part Of &gt; resource ID. </p> </li> <li> <p>Compare the total number of resources in this list with the total number of GIS resources available on the source website. Make a note in the Github issue of how many resources exist in both places. </p> </li> <li> <p>Briefly, visually scan both lists of resources. Note your assessment of what has been added or removed, if anything. You don't need to check every link or make a complete list of changes. Our goal is to evaluate the magnitude of the change.  </p> </li> <li> <p>In the Is Part Of search results, open the first record in a new tab or window. In a comment on the GitHub issue or notes elsewhere, start a numbered list, and put the link/title of this resource as the first item in the list. </p> </li> <li>Check to see if a resource with the same name is present on the source cite. This could be in the main list of resources, or you may need to search the site. Every source cite is organized slightly differently. Note \"resource present on source site\" or \"resource not present on source site\" in the comment.</li> <li>check links in description of geoportal page, if any, and note broken links</li> <li>Make an attempt to find resource elsewhere and if you find that it's been moved, note new location</li> <li>Label this numbered list item in the comment as NO CHANGE, NEEDS CHANGE, or REMOVED.</li> <li>Repeat steps 5-9 for each resource in the Is Part Of search results.</li> <li> <p>Check main page of source site and any additional locations you've found for GIS resources that aren't part of the catalog in the geoportal. Make a list of ADDED resources. </p> </li> <li> <p>we can spot check instead of checking every record if there are more than a certain total number</p> </li> </ol>"},{"location":"arcgis-harvest-policy-2018/","title":"BTAA GDP Accession Guidelines for ArcGIS Open Data Portals","text":"<p><code>Version 1.0 - April 18, 2018</code></p> <p>Deprecated</p> <p>This document has been replaced by the BTAA-GIN Scanning Guidelines for ArcGIS Hub, version 2.0</p>"},{"location":"arcgis-harvest-policy-2018/#overview","title":"OVERVIEW","text":"<p>This document describes the BTAA GDP Accession Guidelines for ArcGIS Open Data Portals, including eligible sites and records, harvest schedules, and remediation work. This policy may change at any time in response to updates to the ArcGIS Open Data Portal platform and/or the BTAA GDP harvesting and remediation processes.</p>"},{"location":"arcgis-harvest-policy-2018/#eligible-sites","title":"ELIGIBLE SITES","text":"<p>Policy: Any ArcGIS Open Data Portal (Arc-ODP) that serves public geospatial data is eligible for inclusion in the BTAA GDP Geoportal (\u201cthe Geoportal\u201d). However, preference is given to portals that are hosting original layers, not federated portals that aggregate from other sites. Task Force members are responsible for finding and submitting Arc-ODPs for inclusion in the Geoportal. Each Arc-ODP will be assigned a Provenance value to the university that submitted it or is closest geographically to the site.</p> <p>Explanation: In order to avoid duplication, records that appear in multiple Arc-ODPs should only be accessioned from one instance.  This also helps to avoid harvesting records that may be out of date or not yet aggregated within federated portals. Although the technical workers at the University of Minnesota will be performing the metadata processing, the Task Force members are expected to periodically monitor their records and make suggestions for edits or additions. </p>"},{"location":"arcgis-harvest-policy-2018/#eligible-records","title":"ELIGIBLE RECORDS","text":"<p>Policy: The only records that will be harvested from Arc-ODPs are Esri REST Services of the type Map Service, Feature Service, or Image Service. This is further restricted to only items that are harvestable through the DCAT API. By default, the following records types will not be accessioned on a regular basis:</p> <ul> <li>Web applications</li> <li>Nonspatial data, tabular data, PDFs</li> <li>Single records that describe many different associated files to download, such as imagery services with a vast number of sublayers</li> </ul> <p>Explanation: Arc-ODPs are structured to automatically create item records from submitted Esri REST services.  However, Arc-ODP administrators are able to manually add records for other types of resources, such as external websites or documents.  These may not be spatial datasets and may not have consistently formed metadata or access links, which impedes automated accessioning. If these types of resources are approved by the Metadata Coordinator, they may be processed separately from the regular accessions.</p>"},{"location":"arcgis-harvest-policy-2018/#query-harvest-frequency","title":"QUERY &amp; HARVEST FREQUENCY","text":"<p>Policy: The Arc-ODPs included in the Geoportal will be queried monthly to check for deleted and new items.  The results of this query will be logged. Deleted items will be removed from the geoportal immediately.  New records from the Arc-ODPs will be accessioned and processed within two months of harvesting the metadata.  </p> <p>Explanation: Removing broken links is a priority for maintaining a positive user experience. However, accessioning and processing records requires remediation work that necessitates a variable time frame.</p>"},{"location":"arcgis-harvest-policy-2018/#remediation-work","title":"REMEDIATION WORK","text":"<p>The records will be processed by the Metadata Coordinator and available UMN Graduate Research Assistants. The following metadata remediation steps will be undertaken:</p>"},{"location":"arcgis-harvest-policy-2018/#1-a-python-script-will-be-run-to-harvest-metadata-from-the-dcat-api-this-will-provide-the-following-elements-for-each-record","title":"1. A Python script will be run to harvest metadata from the DCAT API.  This will provide the following elements for each record:","text":"<ul> <li>Identifier</li> <li>Title</li> <li>Description</li> <li>Date Issued</li> <li>Date Modified</li> <li>Bounding Box</li> <li>Publisher</li> <li>Keywords</li> <li>Landing Page</li> <li>Web Service link</li> </ul>"},{"location":"arcgis-harvest-policy-2018/#2-the-metadata-will-be-batch-augmented-with-administrative-template-values-in-the-following-elements","title":"2. The metadata will be batch augmented with Administrative template values in the following elements:","text":"<ul> <li>Collection</li> <li>Rights</li> <li>Type</li> <li>Format</li> <li>Provenance</li> <li>Language</li> <li>Centroid (derived from Bounding Box)</li> <li>Download Link (created from Landing Page)</li> <li>Tag (web service type)</li> <li>Thumbnail link (derived from server or Arc-ODP page)</li> </ul>"},{"location":"arcgis-harvest-policy-2018/#3-the-metadata-will-be-manually-augmented-with-descriptive-values-for-the-following-elements","title":"3. The metadata will be manually augmented with descriptive values for the following elements:","text":"<ul> <li>Subject (at least one ISO topic category)</li> <li>Geometry type (Vector or Raster)</li> <li>Spatial Coverage (place names written out to the nation level: \u201cMinneapolis, Minnesota, United States\u201d)</li> <li>Temporal Coverage (dates included in the title or description)</li> <li>Title (add place names, expand acronyms, and move dates to the end of the string)</li> <li>Description (remove html and non-UTF8 characters)</li> <li>Creator (if available)</li> <li>Solr Year  (integer value based on temporal coverage or published date)</li> </ul>"},{"location":"arcgis-harvest-policy-2018/#4-the-metadata-will-not-be-fully-remediated-for-the-following-cases","title":"4. The metadata will not be fully remediated for the following cases:","text":"<ul> <li>Missing bounding box coordinates (0.00 values) will be defaulted to the bounding box of administrative level of the Arc-ODP or the record will be omitted.</li> <li>Missing or incomplete descriptions will be left alone or omitted from the record</li> <li>Individual records that require additional research in order to make the metadata record compliant, such as missing required elements or non-functioning links, will be omitted.</li> </ul>"},{"location":"arcgis-harvest-policy-2018/#standards-metadata","title":"STANDARDS METADATA","text":"<p>Policy: Creating or linking to standards based metadata files for Arc-ODPs is out of scope at this time.</p> <p>Explanation: If metadata is enabled for an Arc-ODP, it will be available as ArcGIS Metadata Format 1.0 in XML, which is not a schema that GeoBlacklight can display. The metadata may also be available as FGDC or ISO HTML pages, but these types of links are not part of the current GeoBlacklight schema. Further, very few Arc-ODPs are taking advantage of this feature at this time.</p>"},{"location":"arcgis-hub-guidelines/","title":"BTAA-GIN Scanning Guidelines for ArcGIS Hubs","text":"<p><code>Version 2.0 - April 24, 2023</code></p>"},{"location":"arcgis-hub-guidelines/#overview","title":"Overview","text":"<p>This document describes the BTAA-GIN Scanning Guidelines for ArcGIS Hubs, including eligible sites and records, harvest schedules, and remediation work. This policy may change at any time in response to updates to the ArcGIS Hub architecture platform and/or the BTAA-GIN harvesting and remediation processes.</p>"},{"location":"arcgis-hub-guidelines/#eligible-sites","title":"Eligible sites","text":"<p>Guideline: Any ArcGIS Hub (Hub) that serves public geospatial data is eligible for inclusion in the BTAA Geoportal (\u201cthe Geoportal\u201d). Our scope includes public Hubs from the states in the BTAA geographic region and Hubs submitted by Team Members that are of interest to BTAA researchers. </p> <p>Explanation: See the BTAA-GIN Collection Development Policy for more details.</p>"},{"location":"arcgis-hub-guidelines/#eligible-records","title":"Eligible records","text":"<p>Guideline: The only records that will be harvested from Hubs are Esri REST Services of the type Map Service, Feature Service, or Image Service. This is further restricted to only items that are harvestable through the DCAT API. By default, the following records types will not be accessioned on a regular basis:</p> <ul> <li>Web applications</li> <li>Nonspatial data, tabular data, PDFs</li> <li>Single records that describe many different associated files to download, such as imagery services with a vast number of sublayers</li> </ul> <p>Explanation: Hubs are structured to automatically create item records from submitted Esri REST services.  However, Hub administrators are able to manually add records for other types of resources, such as external websites or documents.  These may not be spatial datasets and may not have consistently formed metadata or access links, which impedes automated accessioning.</p>"},{"location":"arcgis-hub-guidelines/#frequency","title":"Frequency","text":"<p>Guideline: The Hubs included in the Geoportal will be scanned weekly to harvest complete lists of eligible records. The list will be published and overwrite the previous scan.</p> <p>Explanation: Broken links negatively impact user experience. Over the course of a week, as many as 10% of the ArcGIS Hub records in the Geoportal can break or include outdated information. </p>"},{"location":"arcgis-hub-guidelines/#metadata-remediation","title":"Metadata Remediation","text":"<p>Guideline: The harvesting script, <code>R-01_arcgis-hubs</code>, programmatically performs all of the remediation for each record. </p> <p>Explanation: We now scan a large number of ArcGIS Hubs, which makes manual remediation unrealistic.  This is in contrast to the previous policy established in 2018, when our collection was smaller.</p> <p>Info</p> <p>This document replaces the BTAA GDP Accession Guidelines for ArcGIS Open Data Portals, version 1.0.</p>"},{"location":"b1g-custom-elements/","title":"Custom Elements","text":"<p>This page documents the custom metadata elements for the GeoBTAA Metadata Profile. These elements extend the official OpenGeoMetadata (Aardvark) schema.</p> b1g-id Label URI Obligation b1g-01 Code <code>b1g_code_s</code> Required b1g-02 Status <code>b1g_status_s</code> Optional b1g-03 Accrual Method <code>b1g_dct_accrualMethod_s</code> Required b1g-04 Accrual Periodicity <code>b1g_dct_accrualPeriodicity_s</code> Optional b1g-05 Date Accessioned <code>b1g_dateAccessioned_s</code> Required b1g-06 Date Retired <code>b1g_dateRetired_s</code> Conditional b1g-07 Child Record <code>b1g_child_record_b</code> Conditional b1g-08 Mediator <code>b1g_dct_mediator_sm</code> Conditional b1g-09 Access <code>b1g_access_s</code> Conditional b1g-10 Image <code>b1g_image_ss</code> Optional b1g-11 GeoNames <code>b1g_geonames_sm</code> Optional b1g-12 Publication State <code>b1g_publication_state_s</code> Required b1g-13 Language String <code>b1g_language_sm</code> Required b1g-14 Creator ID <code>b1g_creatorID_sm</code> Optional b1g-15 Conforms To <code>b1g_dct_conformsTo_sm</code> Optional b1g-16 Spatial Resolution in Meters <code>b1g_dcat_spatialResolutionInMeters_sm</code> Optional b1g-17 Spatial Resolution as Text <code>b1g_geodcat_spatialResolutionAsText_sm</code> Optional b1g-18 Provenance Statement <code>b1g_dct_provenanceStatement_sm</code> Optional b1g-19 Admin Tags <code>b1g_adminTags_sm</code> Optional"},{"location":"b1g-custom-elements/#code","title":"Code","text":"Label Code URI <code>b1g_code_s</code> Profile ID b1g-01 Obligation Required Multiplicity 1-1 Field type string Purpose To group records based upon their source Entry Guidelines Codes are developed by the metadata coordinator and indicate the provider, the type of institution hosting the resources, and a numeric sequence number. For more details, see Code Naming Schema  Commentary This administrative field is used to group and track records based upon where they are harvested. This is frequently an identical value to \"Member Of\". The value will differ for records that are retired (these are removed from \"Member Of\") and records that are part of a subcollection. Controlled Vocabulary yes-strict Example value 12d-01 Element Set B1G"},{"location":"b1g-custom-elements/#status","title":"Status","text":"Label Status URI <code>b1g_status_s</code> Profile ID b1g-02 Obligation Optional Multiplicity 0-1 Field type string Purpose To indicate if a record is currently active, retired, or unknown. It can also be used to indicate if individual data layers from website has been indexed in the Geoportal. Entry Guidelines Plain text string is acceptable Commentary This is a legacy admin field that was previously used to track published vs retired items. The current needs are still TBD. Controlled Vocabulary no Example value Active Element Set B1G"},{"location":"b1g-custom-elements/#accrual-method","title":"Accrual Method","text":"Label Accrual Method URI <code>b1g_dct_accrualMethod_s</code> Profile ID b1g-03 Obligation Required Multiplicity 1-1 Field type string Purpose To describe how the record was obtained Entry Guidelines Some values, such as \"ArcGIS Hub\" should be entered consistently. Others may be more descriptive, such as \"Manually entered from text file.\" Commentary This allows us to find all of the ArcGIS records in one search. It also can help track records that have been harvested via different methods within the same collection. Controlled Vocabulary no Example value ArcGIS Hub Element Set B1G/ Dublin Core"},{"location":"b1g-custom-elements/#accrual-periodicity","title":"Accrual Periodicity","text":"Label Accrual Periodicity URI <code>b1g_dct_accrualPeriodicity_s</code> Profile ID b1g-04 Obligation Optional Multiplicity 0-1 Field type string Purpose To indicate how often a collection is reaccessioned Entry Guidelines Enter one of the following values: Annually, Semiannually, Quarterly, Monthly, As Needed Commentary This field is primarily for collection level records. Controlled Vocabulary yes-not strict Example value As Needed Element Set B1G/ Dublin Core"},{"location":"b1g-custom-elements/#date-accessioned","title":"Date Accessioned","text":"Label Date Accessioned URI <code>b1g_dateAccessioned_s</code> Profile ID b1g-05 Obligation Required Multiplicity 1-1 Field type string Purpose To store the date a record was harvested Entry Guidelines Enter the date a record was harvested OR when it was added to the geoportal using the format yyyy-mm-dd Commentary This field allows us to track how many records are ingested into the portal in a given time period and to which collections. Controlled Vocabulary no Example value 2021-01-01 Element Set B1G"},{"location":"b1g-custom-elements/#date-retired","title":"Date Retired","text":"Label Date Retired URI <code>b1g_dateRetired_s</code> Profile ID b1g-06 Obligation Conditional Multiplicity 0-1 Field type string Purpose To store the date the record was removed from the geoportal public interface Entry Guidelines Enter the date a record was removed from the geoportal Commentary This field allows us to track how many records have been removed from the geoportal interface by time period and collection. Controlled Vocabulary no Example value 2021-01-02 Element Set B1G"},{"location":"b1g-custom-elements/#child-record","title":"Child Record","text":"Label Child Record URI <code>b1g_child_record_b</code> Profile ID b1g-07 Obligation Optional Multiplicity 0-1 Field type string boolean Purpose To apply an algorithm to the record that causes it to appear lower in search results Entry Guidelines Only one of two values are allowed: true or false Commentary This is used to lower a record's placement in search results. This can be useful for a large collection with many similar metadata values that might clutter a user's experience. Controlled Vocabulary string boolean Example value true Element Set B1G"},{"location":"b1g-custom-elements/#mediator","title":"Mediator","text":"Label Mediator URI <code>b1g_dct_mediator_sm</code> Profile ID b1g-08 Obligation Conditional Multiplicity 0-0 or 1-* Field type string Purpose To indicate the universities that have licensed access to a record Entry Guidelines The value for this field should be one of the names for each institution that have been coded in the GeoBlacklight application. Commentary This populates a facet on the search page so that users can filter to only databases that they are able log into based upon their institutional affiliation. Controlled Vocabulary yes Example value University of Wisconsin-Madison Element Set B1G/ Dublin Core"},{"location":"b1g-custom-elements/#access","title":"Access","text":"Label Access URI <code>b1g_access_s</code> Profile ID b1g-09 Obligation Conditional Multiplicity 0-0 or 1-1 Field type string JSON Purpose To supply the links for restricted records Entry Guidelines The field value is an array of key/value pairs, with keys representing an insitution code and values the URL for the library catalog record. See the Access Template for entry. Commentary This field is challenging to construct manually, is it is a JSON string of institutional codes and URLs. The codes are used instead of the actual names in order to make the length of the field more manageable and to avoid spaces. Controlled Vocabulary no Example value {\\\"03\\\":\\\"https://purl.lib.uiowa.edu/PolicyMap\\\",\\\"04\\\":\\\"https://www.lib.umd.edu/dbfinder/id/UMD09180\\\",\\\"05\\\":\\\"https://primo.lib.umn.edu/permalink/f/1q7ssba/UMN_ALMA51581932400001701\\\",\\\"06\\\":\\\"http://catalog.lib.msu.edu/record=b10238077~S39a\\\",\\\"07\\\":\\\"https://search.lib.umich.edu/databases/record/39117\\\",\\\"09\\\":\\\"https://libraries.psu.edu/databases/psu01762\\\",\\\"10\\\":\\\"https://digital.library.wisc.edu/1711.web/policymap\\\",\\\"11\\\":\\\"https://library.ohio-state.edu/record=b7869979~S7\\\"} Element Set B1G"},{"location":"b1g-custom-elements/#image","title":"Image","text":"Label Image URI <code>b1g_image_ss</code> Profile ID b1g-10 Obligation Optional Multiplicity 0-0 or 0-1 Field type stored string (URL) Purpose To show a thumbnail on the search results page Entry Guidelines Enter an image file using a secure link (https). Acceptable file types are JPEG or PNG Commentary This link is used to harvest an image into the Geoportal server for storage and display. Once it has been harvested, it will remain in storage, even if the orginal link to the image stops working. Controlled Vocabulary no Example value https://gis.allencountyohio.com/GIS/Image/countyseal.jpg Element Set B1G"},{"location":"b1g-custom-elements/#geonames","title":"GeoNames","text":"Label GeoNames URI <code>b1g_geonames_sm</code> Profile ID b1g-11 Obligation Optional Multiplicity 0-* Field type stored string (URI) Purpose To indicate a URI for a place name from the GeoNames database Entry Guidelines Enter a value in the format \"http://sws.geonames.org/<code>URI</code>\" Commentary This URI provides a linked data value for one or more place names. It is optional as there is currently no functionality tied to it in the GeoBlacklight application Controlled Vocabulary yes Example value https://sws.geonames.org/2988507 Element Set B1G"},{"location":"b1g-custom-elements/#publication-state","title":"Publication State","text":"Label Publication State URI <code>b1g_publication_state_s</code> Profile ID b1g-12 Obligation Required Multiplicity 1-1 Field type string Purpose To communicate to Solr if the item is public or hidden Entry Guidelines Use the dropdown or batch editing functions to change the state Commentary When items are first added to GBL Admin, they are set as Draft by default. When they are ready to be published, they can be manually changed to Published. If the record is retired or needs to be hidden, it can be changed to Unpublished Controlled Vocabulary yes Example value Draft Element Set B1G"},{"location":"b1g-custom-elements/#language-string","title":"Language string","text":"Label Language string URI <code>b1g_language_sm</code> Profile ID b1g-13 Obligation Required Multiplicity 1-* Field type string Purpose To display the spelled out string (in English) of a language code to users Entry Guidelines This field is automatically generated from the Language field in the main form Commentary The OGM schema specified using a 3-digit code to indicate lanuage. In order to display this to users, it needs to be translated into a human-readable string. Controlled Vocabulary yes Example value French Element Set B1G"},{"location":"b1g-custom-elements/#creator-id","title":"Creator ID","text":"Label Creator ID URI <code>b1g_creatorID_sm</code> Profile ID b1g-14 Obligation Optional Multiplicity 0-* Field type string Purpose To track the URI of a creator value Entry Guidelines This field is entered as a URI representing an authority record Commentary These best practices recommend consulting one or two name registries when deciding how to standardize names of creators: the Faceted Application of Subject Terminology (FAST) or the Library of Congress Name Authority File (LCNAF). FAST is a controlled vocabulary based on the Library of Congress Subject Headings (LCSH) that is well-suited to the faceted navigation of the Geoportal. The LCNAF is an authoritative list of names, events, geographic locations and organizations used by libraries and other organizations to collocate authorized creator names to make searching and browsing easier. Controlled Vocabulary yes Example value http://id.worldcat.org/fast/2013467 Element Set B1G"},{"location":"b1g-custom-elements/#conforms-to","title":"Conforms To","text":"Label Conforms To URI <code>b1g_dct_conformsTo_sm</code> Profile ID b1g-15 Obligation Optional Multiplicity 0-* Field type string Purpose To display the coordinate reference system Entry Guidelines This field is entered as a URI from a known online reference, such as epsg.io Commentary This field is from Dublin Core. Our usage aligns with the DCAT-3 profile Controlled Vocabulary no Example value https://epsg.io/26915 Element Set B1G/DCAT"},{"location":"b1g-custom-elements/#spatial-resolution-in-meters","title":"Spatial Resolution in Meters","text":"Label Spatial Resolution in Meters URI <code>b1g_dcat_spatialResolutionInMeters_sm</code> Profile ID b1g-16 Obligation Optional Multiplicity 0-* Field type string Purpose To indicate the resolution of a raster dataset Entry Guidelines Enter a number representing the resolution in meters Commentary This is a DCAT-3 field Controlled Vocabulary no Example value 100 Element Set B1G/DCAT"},{"location":"b1g-custom-elements/#spatial-resolution-as-text","title":"Spatial Resolution as Text","text":"Label Spatial Resolution as Text URI <code>b1g_geodcat_spatialResolutionAsText_sm</code> Profile ID b1g-17 Obligation Optional Multiplicity 0-* Field type string Purpose To descriptively indicate the spatial resolution of an item Entry Guidelines Can be a scale, distance measure, or other free text. Commentary This can be used for any resource, but may be most applicable to scanned map records that have a Scale value in the original MARC metadata. Controlled Vocabulary no Example value Scale: 1:24,000 Element Set B1G/GeoDCAT"},{"location":"b1g-custom-elements/#provenance-statement","title":"Provenance Statement","text":"Label Provenance Statement URI <code>b1g_dct_provenanceStatement_sm</code> Profile ID b1g-18 Obligation Optional Multiplicity 0-* Field type string Purpose To indicate where a dataset came from Entry Guidelines Enter plain text. It may include URLs. Commentary This is primarily used for logging accessions, processing activities, and data sources. We are not currently using it to track where metadata was sourced- only where the dataset itself was sourced.  This is a Dublin Core field that has been adopted by DCAT-3. It is a crosswalk from the lineage element in ISO 19139. Controlled Vocabulary yes Example value Downloaded from https://www.nj.gov/transportation/refdata/gis/data.shtm August 19, 2024 Element Set B1G / DCAT"},{"location":"b1g-custom-elements/#admin-tags","title":"Admin Tags","text":"Label Admin Tags URI <code>b1g_adminTags_sm</code> Profile ID b1g-19 Obligation Optional Multiplicity 0-* Field type string Purpose To store local tags to aid in finding and filtering items. Entry Guidelines Enter tags and codes as strings. Examples are for records cleaned during sprints and metadata updates. Commentary This is a custom field that is only useful locally and not intended to be interoperable. Controlled Vocabulary no Example value bbox_cleanupSprint_2024 Element Set B1G"},{"location":"ckan/","title":"Overview of CKAN Data Portals and its APIs","text":"<p>\"CKAN is a tool for making open data websites\". (https://docs.ckan.org/en/2.10/user-guide.html#what-is-ckan) CKAN is often utilized by governments and organizations and is an open-source alternative to platforms like ArcGIS Hubs.</p>"},{"location":"ckan/#content-organization","title":"Content Organization","text":"<p>The content organization model of a CKAN site uses the term Datasets for each item record. A Dataset may have multiple Resources, such as downloadable files, thumbnails, supplemental metadata files, and external links. This model can give data providers flexibility on how they organize their files, but can be challenging for harvesting into the BTAA Geoportal. </p> <p>Unlike CKAN, GeoBlacklight was designed to have only one data file per record, so it can be challenging to programmatically sort through all of the possible access points for a Dataset and attach them to a single record in GeoBlacklight. To mitigate this, we use the multiple downloads option when possible.</p>"},{"location":"ckan/#metadata","title":"Metadata","text":"<p>CKAN metadata contains several basic fields (documented at https://ckan.org/features/metadata) along with an \"extras\" group that can be customized by site. Some portals have many custom fields in \"extras\" and some do not use them at all.</p>"},{"location":"ckan/#api","title":"API","text":"<p>CKAN offers several types of APIs for sharing metadata. The most useful one for the BTAA Geoportal is the <code>package_search</code>, which can be accessed by appending \"api/3/action/package_search\" to a base URL. </p> <p>Example</p> <p>https://demo.ckan.org/api/3/action/package_search</p>"},{"location":"codeNamingSchema/","title":"Code Naming Schema","text":"<p>Each website / collection in the BTAA Geoportal has an alphanumeric code. This code is also added to each metadata record to facilitate administrative tasks and for grouping items by their source. Some of the Codes are randomly generated strings, but most are constructed with an administrative schema described below:</p> First part of string Contributing institution 01 Indiana University 02 University of Illinois Urbana-Campaign 03 University of Iowa 04 University of Maryland 05 University of Minnesota 06 Michigan State University 07 University of Michigan 08 Pennsylvania State University 09 Purdue University 10 University of Wisconsin-Madison 11 The Ohio State University 12 University of Chicago 13 University of Nebraska-Lincoln 14 Rutgers University 15 Northwestern University 16 University of Washington 17 University of Oregon Second part of string Type of organization hosting the datasests a State b County c Municipality d University f Other (ex. NGOs, Regional Groups, Collaborations) g Federal Third part of string The sequence number added in order of accession or a county FIPs code -01 First collection added from same institution and type of organization -02 Second collection added from same institution and type of organization -55079 County FIPS code for Milwaukee County, Wisconsin <p>Example</p> <p>code for a collection sourced from Milwaukee County: '10b-55079'</p>"},{"location":"collection-development-policy/","title":"BTAA Geoportal Collection Development Policy","text":"<p> Authors: BTAA Collection Development &amp; Education Outreach Committee</p>"},{"location":"collection-development-policy/#purpose","title":"Purpose","text":"<p>The BTAA Geospatial Information Network is a collaborative project to enhance discoverability, facilitate access, and connect scholars across the Big Ten Academic Alliance (BTAA) to scanned maps, geospatial data, and aerial imagery resources. The project\u2019s main output is the BTAA Geoportal, which serves as a platform through which participating libraries can share materials from their collections to make them more easily discoverable and accessible to varied user communities. Collections within the Geoportal primarily support the research, teaching, learning, and information needs of faculty, staff, and students at participating institutions and beyond.</p> <p>The project supports the creation and aggregation of discovery-focused metadata describing geospatial resources from participating institutions and public sources across the Big Ten region and makes those resources discoverable via an open source portal. For more information and a list of participating BTAA institutions, please visit our project site.</p>"},{"location":"collection-development-policy/#summary-of-collection-scope","title":"Summary of Collection Scope","text":"<p>Access to the BTAA Geoportal is open to all. This collection consists of geospatial resources relevant to all disciplines. Access to resources is curated based on their authoritativeness, currency, comprehensiveness, ease of use, and relevancy. Materials included are generally publicly available geospatial datasets (vector/raster), scanned maps (georeferenced or with bounding box coordinates), and aerial imagery. Scanned maps protected by copyright are not included in the Geoportal. Access to licensed resources may be restricted to users affiliated with a participating institution.</p> <ol> <li>Geographic areas: Items in the collection vary in scale based on subject and range from global to local. Geographic areas vary based on subject and may refer to biomes/ecosystems, political boundaries, cultural boundaries, economic boundaries, or land use types. In addition to a geographic focus on the Big Ten region (i.e., the states where participating institutions are located), the collection will emphasize resources and topics relevant to faculty and student research interests and reflect the strengths of participating library collections.</li> <li>Time periods: All time periods are collected, with an ability to accommodate both current and historical versions of datasets.</li> <li>Format: The collection consists of geospatial datasets, georeferenced maps, scanned maps with bounding box coordinates, and aerial imagery. Records for web mapping applications may also be included, with priority given to applications with datasets that are also accessible for download through the Geoportal. Preference is given to open and open source formats, but other formats are accepted as required to facilitate ease of use. When possible, resources are presented in formats that allow for download capabilities. Additional software may be needed to view datasets after download.</li> <li>Language(s): The collection primarily consists of English language content. Some non-English language content may be available for certain regions, reflecting the collection strengths and research/curricular interests of participating institutions.</li> <li>Diversity: The Geoportal and its participating institutions aspire to collect and provide access to geospatial resources that represent diverse perspectives, abilities, and experience levels. We will strive to apply best practices for diverse collection development as they relate to geospatial resources, including but not limited to:<ol> <li>considering resources from small, independent, and local producers</li> <li>seeking content created by and representative of marginalized and underrepresented groups.</li> </ol> </li> <li>Preservation and life cycle: Digital file preservation for discovery metadata is managed by BTAA Geoportal staff. Digital file preservation for resources is the responsibility of the content provider. Resources may cease to be accessible through the Geoportal if access from the original provider is no longer available.</li> </ol>"},{"location":"collection-development-policy/#statement-of-communication","title":"Statement of Communication","text":"<p>The members of the Geoportal project team will continue to communicate with the creators of other geoportals (GeoBlacklight Community, etc.), with data providers in our respective regions, and across Big Ten institutions to build a comprehensive and robust collection.</p> <p>Implementation and Revision Schedule: This policy will be reviewed annually by the Collection Development &amp; Education Outreach Committee and is considered effective on the date indicated below. It will be revised as needed to reflect new collection needs and identify new areas of study as well as those areas that may be excluded.</p> <p>Updated: April 27, 2022</p>"},{"location":"curation-priorities/","title":"Curation Priorities","text":"<p> Authors: BTAA Collection Development &amp; Education Outreach Committee; Product Manager</p> <p>There are three distinct but related aspects of prioritizing the addition of new collections: content/theme, administration, and technology. </p> <p>These priorities will affect how quickly the items are processed or where they fall in line within our processing queue.</p>"},{"location":"curation-priorities/#contenttheme","title":"Content/Theme","text":"<p>When it comes to scanned maps, prioritization based on content or theme is primarily a local effort. However, there are opportunities for internal collaborations, including with Special Collections librarians or other local digital collections initiatives. These collaborations can allow for unique and distinctive maps to be harvested into the geoportal across our universities.</p> <p>For geospatial data, datasets created in association with research projects at our institutions may be a high priority based on content or theme. Additionally, resources that provide access to foundational datasets, such as administrative boundaries, parcels, road networks, address points, and land use, should also be considered.</p> <p>Regardless of the content type, special consideration should be given to highly relevant content, especially to current events. For example, in April 2020, a call went out to all task force members to identify and submit content related to COVID-19 for harvesting into the geoportal. Content that aligns with other ongoing BTAA-GIN program efforts, such as the Diverse Collections Working Group, will also be a higher priority as these efforts are further developed.</p>"},{"location":"curation-priorities/#administration","title":"Administration","text":"<p>Collections may be prioritized based on the organization responsible for creating and maintaining content, which impacts the types of maps or datasets available to be harvested, spatial and temporal coverage, and stability. Based on these considerations, current priorities in terms of administration are:</p> <ol> <li> <p>University libraries and archives</p> <ul> <li>Links to these resources are likely to be stable</li> <li>Resources will likely be documented with a metadata standard</li> <li>Represent our core audience</li> </ul> </li> <li> <p>States and counties</p> <ul> <li>Produce most foundational geospatial datasets (e.g., roads and parcels) and are currently our largest source of geospatial data</li> <li>Technology and open data policies vary widely resulting in patchwork coverage</li> </ul> </li> <li> <p>Regional organizations and research institutes</p> <ul> <li>Often special organizations with funding to create geospatial data across political boundaries</li> <li>Higher risk of harvesting duplicate datasets, as these organizations sometimes aggregate records from cities, counties, or state agencies</li> </ul> </li> <li> <p>Cities</p> <ul> <li>less likely to produce and share data in geospatial formats and more likely to share tabular data</li> <li>prioritized cities: major metropolitan areas and the locations of our university campuses</li> </ul> </li> </ol>"},{"location":"curation-priorities/#technology","title":"Technology","text":"<p>The source's hosting platform influences the ease of harvesting, the quality of the metadata, and the stability of the access links. Based on these considerations, current priorities in terms of technology are:</p> <ol> <li> <p>Published via known portal or digital library platforms, including:</p> <ul> <li>Blacklight/GeoBlacklight</li> <li>Islandora</li> <li>Preservica</li> <li>ArcGIS Hubs</li> <li>Socrata</li> <li>CKAN</li> <li>Sites with OAI-PMH enabled APIs</li> </ul> </li> <li> <p>Custom portals</p> <ul> <li>each portal requires a customized script for HTML web parsing</li> <li>writing and maintaining custom scripts takes extra time</li> </ul> </li> <li> <p>Static webpages with download links</p> <ul> <li>at a minimum, a title is required for each item</li> <li>static sites with nested links take a long time to process and may require an extensive amount of manual work</li> </ul> </li> <li> <p>Database websites</p> <ul> <li>require the user to perform interactive queries to extract data</li> <li>not realistic to make Geoportal records for individual datasets</li> <li>usually results in a single \"website\" record in the Geoportal to represent the database</li> </ul> </li> </ol>"},{"location":"dcat/","title":"DCAT Metadata","text":""},{"location":"dcat/#overview","title":"Overview","text":"<p>DCAT (Data Catalog Vocabulary) is metadata schema for web-based data catalogs. It is intended to facilitate interoperability and many data platforms offer a DCAT API for metadata sharing.</p> <p>The most up-to-date documentation of the schema can be found here: https://www.w3.org/TR/vocab-dcat-3/</p> <p>Documentation that is older, but still in use for United States portals can be found here: https://resources.data.gov/resources/dcat-us/</p>"},{"location":"dcat/#json-structure","title":"JSON Structure","text":"<p>Many of the data platforms in the United States use a DCAT profile documented as \"Project Open Data Catalog\".  The following JSON template shows the generic structure of a DCAT JSON document:</p> <pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n  \"id\": \"https://project-open-data.cio.gov/v1.1/schema/catalog.json#\",\n  \"title\": \"Project Open Data Catalog\",\n  \"description\": \"Validates an entire collection of Project Open Data metadata JSON objects. Agencies produce said collections in the form of Data.json files.\",\n  \"type\": \"object\",\n  \"dependencies\": {\n    \"@type\": [\n      \"@context\"\n    ]\n  },\n  \"required\": [\n    \"conformsTo\",\n    \"dataset\"\n  ],\n  \"properties\": {\n    \"@context\": {\n      \"title\": \"Metadata Context\",\n      \"description\": \"URL or JSON object for the JSON-LD Context that defines the schema used\",\n      \"type\": \"string\",\n      \"format\": \"uri\"\n    },\n    \"@id\": {\n      \"title\": \"Metadata Catalog ID\",\n      \"description\": \"IRI for the JSON-LD Node Identifier of the Catalog. This should be the URL of the data.json file itself.\",\n      \"type\": \"string\",\n      \"format\": \"uri\"\n    },\n    \"@type\": {\n      \"title\": \"Metadata Context\",\n      \"description\": \"IRI for the JSON-LD data type. This should be dcat:Catalog for the Catalog\",\n      \"enum\": [\n        \"dcat:Catalog\"\n      ]\n    },\n    \"conformsTo\": {\n      \"description\": \"Version of Schema\",\n      \"title\": \"Version of Schema\",\n      \"enum\": [\n        \"https://project-open-data.cio.gov/v1.1/schema\"\n      ]\n    },\n    \"describedBy\": {\n      \"description\": \"URL for the JSON Schema file that defines the schema used\",\n      \"title\": \"Data Dictionary\",\n      \"type\": \"string\",\n      \"format\": \"uri\"\n    },\n    \"dataset\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"dataset.json\",\n        \"minItems\": 1,\n        \"uniqueItems\": true\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"dcat/#how-to-find-the-dcat-api","title":"How to find the DCAT API","text":"<p>Most sites, including Socrata:</p> <p>To find a data API, a good place to start is to try appending the string \"/data.json\" to the base URL.  If available, your browser will display the data catalog as a JSON file.</p> <p>ArcGIS Hubs:</p> <ul> <li>Version 1: append the string \"/api/feed/dcat-us/1.1.json\". Esri made this change was made in 2022 to differentiate the older DCAT version from 2.0. Our harvest recipe current uses this version.</li> <li>Version 2: use the string \"api/feed/dcat-ap/2.0.1.json\". We plan to evaluate the newer format and will consider migrating our recipe in 2024.</li> </ul>"},{"location":"editingTemplate/","title":"Editing Template","text":"<p>The GeoBTAA Metadata Template (https://z.umn.edu/b1g-template) is a set of spreadsheets that are formatted for our metadata editor, GBL Admin. </p> <p>Users will need to make a copy of the spreadsheet to use for editing.  In some cases, the Metadata Coordinator can provide a customized version of the sheets for specific collections.</p> <p>The Template contains the following tabs:</p> <ul> <li> <code>Primary Template minimal</code> This tab provides the essential fields for most collections. </li> <li> <code>Primary Template full</code> This tab contains all of fields in our profile and is useful for records with more extensive metadata. </li> <li> <code>Distributions Template</code> This tab is for each resource's links, including landing pages, downloads, web services, and supplemental documentation. </li> <li> <code>Field Definitions</code> This lists all of the fields, their definitions, and key attributes. </li> <li> <code>Licensed Access Links</code> A special template for uploading access links for licensed databases </li> <li> <code>Controlled Vocabulary</code> A list of the acceptable values for certain fields. This tab powers the dropdown fields in the templates. </li> </ul> <p>Note</p> <p>The input format for some fields in this template may differ from how the field is documented in OpenGeoMetadata. These differences are intended to make it easier to enter values, which will be transformed when we upload the record to GBL Admin.</p> <ul> <li> <p>Bounding Box coordinates should be entered as <code>W,S,E,N</code>. The coordinates are automatically transformed to a different order <code>ENVELOPE(W,E,N,S)</code>. Read more under the Local Input Guidelines.</p> </li> <li> <p>Date Range should be entered as <code>yyyy-yyyy</code>. This is automatically transformed to [yyyy TO yyyy].</p> </li> </ul>"},{"location":"ephemeral-data/","title":"The challenge of ephemeral data","text":"<p>summary</p> <p>Many of the resources in the BTAA Geoportal are from sites that continually update their datasets. As a result, we need to regularly re-harvest the metadata. </p> <p>Government agencies now issue most geospatial information as digital data instead of as physical maps. However, many academic libraries have not yet expanded their collection scopes to include publicly available digital data, and are therefore no longer systematically capturing and storing the changing geospatial landscape for future researchers. </p> <p>The BTAA Geoportal partially fills a gap in the geospatial data ecosystem by cataloging metadata records for current publicly available state, county, and municipal geospatial resources. The value of this data is high, as researchers routinely use it to form the base layers for web maps or geographic analysis. However, the the mandates and policies for providing this data varies considerably from state to state and from county to county. The lack of consistent policies at this level of government means that this data can be considered ephemeral, as providers regularly migrate, update, delete, and re-publish data without saving previous versions and without notification to the public.</p> <p>The lack of standard policies at this level of government means that this data can be considered ephemeral. It may be updated, removed, or replaced without notification to the public. The rate at which datasets change or disappear is variable, but is often high.</p> <p>This continual turnover creates a difficult environment for researchers to properly source data and replicate results. It also requires a great deal of dedicated labor to maintain the correct access links in the geoportal. As the geoportal\u2019s collection grows, the labor required to maintain it grows as well.</p>"},{"location":"geobtaa-metadata-application-profile/","title":"GeoBTAA Metadata Profile","text":"<p>The GeoBTAA Metadata Application Profile consists of the following components:</p>"},{"location":"geobtaa-metadata-application-profile/#1-opengeometadata-elements","title":"1. OpenGeoMetadata Elements","text":"<ul> <li>The BTAA Geoportal uses the OpenGeoMetadata Schema for each resource. </li> <li>The current version of OpenGeoMetadata is called 'Aardvark'. </li> <li>This lightweight schema was designed specifically for the GeoBlacklight application and is geared towards discoverability. </li> <li>The GeoBTAA Metadata Profile aligns with all of the guidelines and recommendations in the official OpenGeoMetadata documentation. </li> <li>The schema is documented on the OpenGeoMetadata website . </li> </ul>"},{"location":"geobtaa-metadata-application-profile/#2-custom-elements","title":"2. Custom Elements","text":"<ul> <li>The GeoBTAA profile includes custom fields for lifecycle tracking and administration</li> <li>These elements are generally added to the record by admin staff. When they appear on editing templates, they are grayed out.</li> <li>They all start with the namespace <code>b1g</code></li> <li>See the Custom Elements page for more detail</li> </ul>"},{"location":"geobtaa-metadata-application-profile/#3-geobtaa-input-guidelines","title":"3. GeoBTAA Input Guidelines","text":"<ul> <li> <p>For the content in some fields, the GeoBTAA profile has specific guidelines that extends or differs from what is documented in the OpenGeoMetadata schema.</p> </li> <li> <p>See the GeoBTAA Input Guidelines page for more detail</p> </li> </ul> <p>Info</p> <p>The GeoBTAA Metadata Template can be found at https://z.umn.edu/b1g-template</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#python-and-scripting","title":"Python and scripting","text":""},{"location":"glossary/#apis","title":"APIs","text":"<p>An API (Application Programming Interface) is a set of rules, protocols, and tools for building software applications. It specifies how different software components should interact with each other, allowing them to communicate and exchange information.  </p> <p>In the context of Python, APIs are often used to retrieve data from a web server or to interact with an external service. For example, the requests library is a popular Python package that simplifies making HTTP requests to APIs, while the json module provides an easy way to parse and encode JSON data.</p>"},{"location":"glossary/#beautiful-soup","title":"Beautiful Soup","text":"<p>HTML and XML parser</p>"},{"location":"glossary/#conda","title":"Conda","text":"<ul> <li>Conda is an open source package management system and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.</li> </ul>"},{"location":"glossary/#conda-package-manager","title":"Conda Package Manager","text":"<ul> <li>Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment.</li> </ul> <p>For more information on Conda and environments, refer to this website:  https://docs.conda.io/projects/conda/en/stable/user-guide/index.html</p>"},{"location":"glossary/#pandas","title":"Pandas","text":"<p>Pandas is a Python library that contains many functions for analyzing data. For the GeoBTAA workflows, we are most interested in how it eases transformations between JSON and CSV files:</p> <p>CSV files: Pandas can easily read and write CSV files using its <code>read_csv()</code> and <code>to_csv()</code> methods, respectively. These methods can handle many CSV formats, including different delimiter characters, header options, and data types. Once the CSV data is loaded into a Pandas DataFrame, it can be easily manipulated and analyzed using Pandas' powerful data manipulation tools, such as filtering, grouping, and aggregation.</p> <p>JSON data: Pandas can also read and write JSON data using its <code>read_json()</code> and <code>to_json()</code> methods. These methods can handle various JSON formats, such as normal JSON objects, JSON arrays, and JSON lines. Once the JSON data is loaded into a Pandas DataFrame, it can be easily manipulated and analyzed using the same data manipulation tools used for CSV data.</p> <p>pandas DataFrame A DataFrame is similar to a Python list or dictionary, but it has rows and columns, similar to a spreadsheet. This makes it a simpler task to convert between JSON and CSV. To review these Python terms, refer to the glossary.</p>"},{"location":"glossary/#pandas-dataframe","title":"Pandas DataFrame","text":"<p>Pandas DataFrame is a 2-dimensional table-like data structure that is used for data manipulation and analysis. It is a powerful tool for handling and processing structured data. A DataFrame has rows and columns, similar to a spreadsheet. It can contain heterogeneous data types and can be indexed and sliced in various ways. It is part of the Pandas library and provides powerful features for data analysis and manipulation. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas-dataframe</p>"},{"location":"glossary/#python-list","title":"Python List","text":"<p>A list is a basic data structure in Python that is used to store a collection of items of different data types. It is an ordered collection of elements, and each element is indexed by an integer starting from 0. A list can contain elements of different data types, including other lists and dictionaries. A list is mutable, meaning its elements can be added, removed, or modified. It is a simple, general-purpose data structure that is commonly used for storing and manipulating small to medium-sized data sets.</p>"},{"location":"glossary/#python-dictionary","title":"Python Dictionary","text":"<p>A dictionary is another data structure in Python that is used to store data in the form of key-value pairs. It is an unordered collection of elements, where each element is identified by a unique key instead of an index. The keys can be of any hashable data type, and the values can be of any data type. A dictionary is also mutable, meaning its elements can be added, removed, or modified. It is commonly used to store and manipulate structured data, such as user profiles or configuration settings.</p>"},{"location":"glossary/#python-objects","title":"Python Object(s)","text":"<p>In Python, everything is an object. An object is an instance of a class, which is a blueprint for creating objects. It contains data and functions (also called methods) that operate on that data. Objects are created dynamically, which means that you don't have to declare the type of a variable or allocate memory for it explicitly. When you assign a value to a variable, Python creates an object of the appropriate type and associates it with that variable.</p> <p>For example, an integer in Python is an object of the int class, and a string is an object of the str class. Each object of a class has its own set of data attributes, which store the values of its properties, and methods, which operate on those properties.</p>"},{"location":"glossary/#python-interface","title":"Python Interface","text":"<p>In Python, an interface refers to the set of methods that a class or an object exposes to the outside world. It defines the way in which an object can be interacted with, and the methods that are available to be called. An interface can be thought of as a contract that specifies how a class can be used, and what methods are available to a programmer when working with that class.</p> <p>Python is an object-oriented programming language, and as such, it supports the concept of an interface. Python does not have a specific language construct for creating an interface. Instead, interfaces are implemented using a combination of abstract base classes and duck typing.</p> <p>An abstract base class is a class that cannot be instantiated directly, but instead, is intended to be subclassed. It defines a set of abstract methods that must be implemented by any subclass. This allows for the creation of a common interface that can be shared among multiple classes.</p> <p>Duck typing is a concept in Python that allows for the determination of an object's type based on its behavior, rather than its actual type. This means that if an object behaves like a certain type, it is considered to be of that type. This allows for more flexibility in programming, as it allows for the creation of classes that can be used interchangeably, as long as they implement the same methods.</p>"},{"location":"glossary/#spacy","title":"SpaCy","text":"<p>SpaCy is a python module that uses natural language processing (NLP) to comb through text help us find and extract patterns. To extract place names, it uses named entity recognition (NER) by searching a list of place names. This list is called \"GPE\", which stands for Geopolitical Entity.</p> <p>This article in the Code4Lib journal, From Text to Map: Combing Named Entity Recognition and Geographic Information Systems, explains the process we use to extract place names.</p>"},{"location":"glossary/#data-portals","title":"Data Portals","text":""},{"location":"glossary/#arcgis-hubs","title":"ArcGIS Hubs","text":"<p>ArcGIS Hub is a data portal platform that allows users to share geospatial data as web services. It is especially popular with local governments that already use the Esri ArcGIS ecosystem.</p>"},{"location":"glossary/#ckan","title":"CKAN","text":"<p>CKAN is a tool for making open data websites, it helps you manage and publish collections of data.  For CKAN purposes, data is published in units called \u201cdatasets\u201d.</p> <ul> <li>A dataset contains two things:</li> </ul> <p>Information or \u201cmetadata\u201d about the data. For example, the title and publisher, date, what formats it is available in, what license it is released under, etc.</p> <p>A number of \u201cresources\u201d, which hold the data itself. CKAN does not mind what format the data is in. A resource can be a CSV or Excel spreadsheet, XML file, PDF document, image file, linked data in RDF format, etc. CKAN can store the resource internally, or store it simply as a link, the resource itself being elsewhere on the web. A dataset can contain any number of resources. For example, different resources might contain the data for different years, or they might contain the same data in different formats.</p>"},{"location":"glossary/#socrata","title":"Socrata","text":""},{"location":"glossary/#geoblacklight","title":"GeoBlacklight","text":""},{"location":"glossary/#metadata-standards","title":"Metadata Standards","text":"<p>This chart provides links to documentation about the common metadata standards and schemas we encounter when harvesting.</p> Standard Documentation link Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 ArcGIS 1.0 https://desktop.arcgis.com/en/arcmap/10.3/manage-data/metadata/the-arcgis-metadata-format.htm nan nan nan nan nan nan nan DCAT https://www.w3.org/TR/vocab-dcat-3 nan nan nan nan nan nan nan Dublin Core https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ nan nan nan nan nan nan nan FGDC https://www.fgdc.gov/dataandservices/fgdcmeta#:~:text=The%20FGDC%20metadata%20standard%20is,overall%20structure%20to%20the%20standard. nan nan nan nan nan nan nan ISO 19139 https://www.iso.org/standard/67253.html nan nan nan nan nan nan nan MARC https://www.loc.gov/marc/ nan nan nan nan nan nan nan MODS https://www.loc.gov/standards/mods/mods-overview.html nan nan nan nan nan nan nan <ul> <li>Dublin Core: A set of metadata elements that are used to describe resources in a simple and standardized way. Dublin Core is widely used in library systems, archives, and other digital repositories.</li> <li>MODS (Metadata Object Description Schema): A flexible and extensible XML schema for describing a wide range of resources, including books, articles, and other types of digital content.</li> <li>METS (Metadata Encoding and Transmission Standard): A standard for encoding descriptive, administrative, and structural metadata for digital objects.</li> <li>MARC (Machine-Readable Cataloging): A metadata format used by libraries to describe bibliographic information about books, journals, and other materials.</li> </ul>"},{"location":"input-guidelines/","title":"Local Input Guidelines","text":"<p>For the following elements, the GeoBTAA Metadata Profile has input guidelines beyond what is documented in the OpenGeoMetadata schema:</p>"},{"location":"input-guidelines/#title","title":"Title","text":"<p>Maps: The title for scanned maps is generally left as it was originally cataloged by a participating library. MARC subfields are omitted and can be inserted in the Description field.</p> <p>Datasets: Harvested datasets often are lacking descriptive titles and may need to be augmented with place names. Dates may also be added to the end, but if the dataset is subject to updates, the data should be left off.  Acronyms should be spelled out. The preferred format for dataset titles is: <code>Theme [place] {date}</code>. This punctuation allows for batch processing and splitting title elements.</p>"},{"location":"input-guidelines/#language","title":"Language","text":"<p>Although Language is optional in the OGM schema, a three-digit code is required for the BTAA Geoportal.</p>"},{"location":"input-guidelines/#creator","title":"Creator","text":"<p>When possible, Creators should be drawn from a value in the Faceted Application of Subject Terminology (FAST).</p>"},{"location":"input-guidelines/#creator-id","title":"Creator ID","text":"<p>If the Creator value is from a name authority, insert the ID in this field.</p>"},{"location":"input-guidelines/#publisher","title":"Publisher","text":"<p>Maps: Publisher values for maps are pulled from the original catalog record. Remove subfields for place names and dates.</p> <p>Datasets: The BTAA Geoportal does not use the Publisher field for Datasets.</p>"},{"location":"input-guidelines/#provider","title":"Provider","text":"<p>This is the name of the organization hosting the resources. If the organization is part of the BTAA library network, a university icon will display next to the resource's title. However, most Providers will not have an icon.</p>"},{"location":"input-guidelines/#subject","title":"Subject","text":"<p>The OGM Aardvark profile recommends using a controlled vocabulary for subject terms, but does not specify one.  For the BTAA Geoportal, subjects should be in the format used by the Faceted Application of Subject Terminology (FAST).</p>"},{"location":"input-guidelines/#spatial-coverage","title":"Spatial Coverage","text":"<p>Spatial Coverage (place names) should be in the format used by the Faceted Application of Subject Terminology (FAST).  </p> <p>For US counties and cities, the format should be <code>state--county</code> or <code>state--city</code>. The state itself should also be included. Examples:</p> <p>Example</p> <ul> <li> <p>Wisconsin--Dane County</p> </li> <li> <p>Wisconsin--Madison</p> </li> <li> <p>Wisconsin</p> </li> </ul> <p>To facilitate searching, this field should include values up to the state level. For example, include both <code>Wisconsin--Dane County</code> and <code>Wisconsin</code></p>"},{"location":"input-guidelines/#bounding-box","title":"Bounding Box","text":"<p>On the Metadata Editing Template, provide Bounding Boxes in this format: W,S,E,N This order matches the DCAT API and is how the Klokan Bounding Box provides coordinates with their \"CSV\" setting.</p> <p>This format will be programmatically converted to other formats when it is published to the Geoportal:</p> <ul> <li> <p>The OpenGeoMetadata Bounding Box field (<code>dcat_bbox_s</code>) uses this order: <code>ENVELOPE(W,E,N,S)</code></p> </li> <li> <p>The OpenGeoMetadata Geometry field (<code>locn_geometry</code>) uses a WKT format and the coordinate order will be converted to this layout: <code>POLYGON((W N, E N, E S, W S, W N))</code></p> </li> <li> <p>The OpenGeoMetadata Centroid field (<code>dcat_centroid</code>) will be calculated to display longitude,latitude.</p> </li> </ul> <p>Example</p> <p>Metadata CSV: -120,10,-80,35</p> <p>converts to</p> <p><code>dcat_bbox_s:</code> <code>ENVELOPE(-120,-80,35,10)</code></p> <p><code>locn_geometry:</code> <code>POLYGON((-120 35, -80 35, -80 10, -120 10, -120 35))</code></p> <p><code>dcat_centroid</code>: <code>\"22.5,-100.0\"</code></p>"},{"location":"lifecycle/","title":"Lifecycle","text":"<p>Deprecation</p> <p>This lifecycle documentation had been replaced by a newer version at Resource Lifecycle.</p> <p></p>"},{"location":"lifecycle/#1-submit-records","title":"1. Submit Records","text":"<p>It is the role of the Team members to seek out new content for the geoportal. See the page How to Submit Resources to the BTAA Geoportal for more information.</p>"},{"location":"lifecycle/#2-metadata-transition","title":"2. Metadata Transition","text":"<p>This stage involves batch processing of the records, including harvesting, transformations, crosswalking information. This stage is carried out by the Metadata Coordinator, who may contact Team members for assistance.</p> <p>Regardless of the method used for acquiring the metadata, it is always transformed into a spreadsheet for editing. These spreadsheets are uploaded to GBL Admin Metadata Editor.</p> <p>Because of the variety of platforms and standards, this process can take many forms. The Metadata Coordinator will contact Team members if they need to supply metadata directly. </p>"},{"location":"lifecycle/#3-edit-records","title":"3. Edit Records","text":"<p>Once the metadata is in spreadsheet form, it is ready to be normalized and augmented. UMN Staff will add template information and use spreadsheet functions or scripts to programmatically complete the metadata records.</p> <ul> <li>The GeoBTAA Metadata Template is for creating GeoBlacklight metadata.</li> <li>Refer to the documentation for the OpenGeoMetadata, version Aardvark fields and the GeoBTAA Custom Elements for guidance on values and formats.</li> </ul>"},{"location":"lifecycle/#4-publish-records","title":"4. Publish Records","text":"<p>Once the editing spreadsheets are completed, UMN Staff uploads the records to <code>GBL Admin</code>, a metadata management tool. GBL Admin validates records and performs any needed field transformations. Once the records are satisfactory, they are published and available in the BTAA Geoportal.</p> <p>Read more on the GBL Admin documentation page.</p>"},{"location":"lifecycle/#5-maintenance","title":"5. Maintenance","text":"<p>General Maintenance</p> <p>All project team members are encouraged to review the geoportal records assigned to their institutions periodically to check for issues. Use the feedback form at the top of each page in the geoportal to report errors or suggestions.  This submission will include the URL of the last page you were on, and it will be sent to the Metadata Coordinator.</p> <p>Broken Links</p> <p>The geoportal will be programmatically checked for broken links on a monthly basis. Systematic errors will be fixed by UMN Staff. Some records from this report may be referred back to Team Members for investigating broken links.</p> <p>Subsequent Accessions</p> <ul> <li>Portals that utilize the DCAT metadata standard will be re-accessioned monthly.</li> <li>Other GIS data portals will be periodically re-accessioned by the Metadata Coordinator at least once per year.</li> <li>Team members may be asked to review this work and provide input on decisions for problematic records.</li> </ul> <p>Retired Records</p> <p>When an external resource has been moved, deleted, or versioned to a new access link, the original record is retired from the BTAA Geoportal. This is done by converting the Publication State of the record from 'Published' to 'Unpublished'. The record is not deleted from the database and can still be accessed via a direct link. However, it will not show up in any search queries.</p>"},{"location":"model/","title":"Content Organization Model","text":"<p>GeoBlacklight organizes records with a network model rather than with a hierarchical model. It is a flat system whereby every database entry is a \"Layer\" and uses the same metadata fields. Unlike many digital library applications, it does not have different types of records for entities such as \"communities,\" \"collections,\" or \"groups.\" As a result, it does not present a breadcrumb navigation structure, and all records appear in the same catalog directory with the URL of https:geo.btaa.org/catalog/<code>ID</code>.</p> <p>Instead of a hierarchy, GeoBlacklight relates records via metadata fields. These fields include <code>Member Of</code>, <code>Is Part Of</code>, <code>Is Version Of</code>, <code>Source</code>, and a general <code>Relation</code>. This flexibility allows records to be presented in several different ways. For example, records can have multiple parent/child/grandchild/sibling relationships. In addition, they can be nested (i.e., a collection can belong to another collection). They can also connect data layers about similar topics or represent different years in a series.</p> <p>The following diagram illustrates how the BTAA Geoportal organizes records. The connecting arrow lines indicate the name of the relationship. The labels reflect each record's Resource Class (Collections, Websites, Datasets, Maps, Web services).</p> <p></p>"},{"location":"resource-lifecycle/","title":"Resource Lifecycle","text":"<p>The Resource Lifecycle describes how we manage resources from curation to ongoing maintenance.</p> <p></p>"},{"location":"resource-lifecycle/#1-identify","title":"1. Identify","text":"<p> BTAA-GIN Team Members and Product Manager </p> <p>Team members seek out new content for the geoportal. See the page How to Submit Resources to the BTAA Geoportal for more information.</p>"},{"location":"resource-lifecycle/#2-obtain-and-process-metadata","title":"2. Obtain and Process Metadata","text":""},{"location":"resource-lifecycle/#a-harvest","title":"a. Harvest","text":"<p> Graduate Research Assistants and Product Manager </p> <p>This stage involves obtaining the metadata for resources.  At a minimum, this will include a title and and access link. However, it will ideally also include descriptions, dates, authors, rights, keywords, and more. </p> <p>Here are the most common ways that we obtain the metadata:</p> <ol> <li>a BTAA-GIN Team Member sends us the metadata values as individual documents or as a combined spreadsheet</li> <li>we are provided with (or are able to find) an API that will automatically generate the metadata in a structured file, such as JSON or XML</li> <li>we develop a customized script to scrape directly from the HTML on a source's website</li> <li>we manually copy and paste the metadata into a spreadsheet</li> <li>a combination of one or more of the above</li> </ol>"},{"location":"resource-lifecycle/#b-crosswalk","title":"b. Crosswalk","text":"<p>This step involves using a crosswalk to convert the metadata into the schema needed for the Geoportal. Our goal is to end up with a spreadsheet containing columns matching our metadata template.</p> <p>Why do we rely on CSV?</p> <p>CSV (Comma Separated Values) files organize tabular data in plain text format, where each row of data is separated by a line break, and each column of data is separated by a delimiter.</p> <p>We have found this tabular format to be the most human-readable way to batch create, edit, and troubleshoot metadata records. We can visually scan large numbers of records at once and normalize the values in ways that would be difficult with native nested formats, like JSON or XML. Therefore, many of our workflow processes involve transforming things to and from CSV.</p>"},{"location":"resource-lifecycle/#c-edit","title":"c. Edit","text":"<p> Graduate Research Assistants and Product Manager </p> <p>When working with metadata, it is common to come across missing or corrupted values, which require troubleshooting and manual editing in our spreadsheets. Refer to the Collections Project Board for examples of this work.</p>"},{"location":"resource-lifecycle/#d-validate","title":"d. Validate","text":"<p>After compiling the metadata, we run a validation and cleaning script to ensure the records conform to the required elements of our schema. </p>"},{"location":"resource-lifecycle/#3-index-metadata","title":"3. Index Metadata","text":"<p> Product Manager </p>"},{"location":"resource-lifecycle/#a-ingest-to-gbl-admin","title":"a. Ingest to GBL Admin","text":"<p>We upload the completed spreadsheet to GBL Admin, which serves as the administrative interface for the Geoportal. If GBL Admin detects any formatting errors, it will issue a warning and may reject the upload.</p>"},{"location":"resource-lifecycle/#b-publish-new-records-to-the-geoportal","title":"b. Publish new records to the Geoportal","text":"<p>Once the metadata is successfully uploaded to GBL Admin, we can publish the records to the Geoportal. The technology that actually stores the records and enables searching is called Solr. </p>"},{"location":"resource-lifecycle/#c-unpublish","title":"c. Unpublish","text":"<p>Periodically, we need to remove records from the Geoportal. To do this, we use GBL Admin to either delete them or change their status to \"unpublished.\"</p>"},{"location":"resource-lifecycle/#4-maintenance","title":"4. Maintenance","text":"<p> BTAA-GIN Team Members, Graduate Research Assistants, and Product Manager </p>"},{"location":"resource-lifecycle/#a-monitor-sources","title":"a. Monitor sources","text":"<p>We monitor our sources to check for new and retired contnet.</p>"},{"location":"resource-lifecycle/#b-monitor-geoportal","title":"b. Monitor Geoportal","text":"<p>We regularly assess currentness of the content in the Geoportal and check for broken links.</p>"},{"location":"resource-lifecycle/#c-schedule-re-harvests","title":"c. Schedule re-harvests","text":"<p>We schedule re-harvests from sources based on how frequently they update their content. See the Collections Dashboard for this schedule.</p>"},{"location":"resource-lifecycle/#sequence-diagram-of-processing-workflow","title":"Sequence diagram of processing workflow","text":"<p>This diagram illustrates the roles, tasks, and communication workflow for Team Members, the Product Mananger, and Research Assistants. The two technologies are GitHub for tracking work and GBL Admin for managing the metadata records.</p> <p></p>"},{"location":"resourceClasses/","title":"Resource Classes","text":""},{"location":"resourceClasses/#collections","title":"Collections","text":"<p>The BTAA Geoportal interprets the Resource Class, Collections, as top-level, custom groupings. These reflect our curation activities and priorities.</p> <p>Other records are linked to Collections using the <code>Member Of</code> field. The ID of the parent record is added to the child record only. View all of the current Collections in the geoportal at this link:  https://geo.btaa.org/?f%5Bgbl_resourceClass_sm%5D%5B%5D=Collections</p>"},{"location":"resourceClasses/#websites","title":"Websites","text":"<p>The BTAA Geoportal uses the Resource Class, Websites, to create parent records for data portals, digital libraries, dashboards, and interactive maps. These often start off as standalone records. Once the items in a website have been indexed, they will have child records.</p> <p>Individual Datasets, Maps, or Web services  are linked to the Website they came from using the <code>Is Part Of</code> field. The ID of the parent record is added to the child record only.</p> <p>View all of the current Websites in the geoportal at this link: https://geo.btaa.org/?f%5Bgbl_resourceClass_sm%5D%5B%5D=Websites</p>"},{"location":"resourceClasses/#datasets-maps-and-web-services","title":"Datasets, Maps, and Web services","text":"<p>The items in this Resource Class represent individual data layers, scanned map files, and/or geospatial web services. (Some items may have multiple Resource Classes attached to the same record.)</p> <p>This item class is likely to have the most relationships specified in the metadata. A typical Datasets record might have the following: </p> <ol> <li><code>Member Of</code> a Collections record</li> <li><code>Is Part Of</code> a Websites record</li> <li>If the data was digitized from a paper map in the geoportal, it can be linked to the Maps record via the <code>Source</code> relation</li> <li>a general <code>Relation</code> to a research guide or similar dataset</li> </ol>"},{"location":"resourceClasses/#multipart-items","title":"Multipart Items","text":"<p>Many items in the geoportal are multipart. There may be individual pages from an atlas, sublayers from a larger project, or datasets broken up into more than one download. In these cases, the <code>Is Part Of</code> field is used. </p> <p>As a result, these items may have multiple <code>Is Part Of</code> relationships- (1) the parent for the multipart items and (2) the original website.</p>"},{"location":"schedule/","title":"Harvesting Schedule","text":"<p>Info</p> <ul> <li>This schedule was established April 2023</li> <li>Updated February 2025</li> </ul>"},{"location":"schedule/#weekly","title":"Weekly","text":"<ul> <li>ArcGIS Hubs</li> </ul>"},{"location":"schedule/#monthly","title":"Monthly","text":"<ul> <li>Minnesota Geospatial Commons</li> </ul>"},{"location":"schedule/#quarterly","title":"Quarterly","text":"<ul> <li>Illinois Geospatial Data Clearinghouse</li> <li>PASDA</li> <li>Geodata@Wisconsin</li> <li>Humanitarian Data Exchange</li> <li>CKAN portals</li> <li>Socrata</li> </ul>"},{"location":"schedule/#annually","title":"Annually","text":"<ul> <li>Licensed Resources</li> <li>Custom HTML sites for public data</li> <li>Smaller data portals that are updated infrequently</li> </ul>"},{"location":"schedule/#as-needed","title":"As Needed","text":"<ul> <li>Any website that reports errors during broken link scans.</li> <li>Any website when we receive notification that new records are available. </li> </ul> <p>Info</p> <p>See the GitHub Project Board, Collections, to track harvests.</p>"},{"location":"submit-resources/","title":"How to submit resources to the BTAA Geoportal","text":""},{"location":"submit-resources/#1-identify-resources","title":"1. Identify Resources","text":"<p>Places to find public domain collections</p> <ul> <li>State GIS clearinghouses</li> <li>State agencies (especially DNRs and DOTs)</li> <li>County or city GIS departments</li> <li>Library digital collections</li> <li>Research institutes</li> <li>Nonprofit organizations</li> </ul> <p>Review the Curation Priorites and the Collection Development Policy for guidelines on selecting resources.</p>"},{"location":"submit-resources/#optional-contact-the-organization","title":"Optional: Contact the organization","text":"<p>Use this template to inform the organization that we plan to include their resources in our geoportal.</p> <p>Tip</p> <p>If metadata for the resources are not readily available, the organization may be able to send you an API, metadata documents, or a spreadsheet export.</p>"},{"location":"submit-resources/#2-investigate-metadata-harvesting-options","title":"2. Investigate metadata harvesting options","text":"<p>Metadata records can be submitted directly or we can harvest it using parsing and transformation scripts. </p> <p>Here are the most common methods of obtaining metadata for the BTAA Geoportal:</p>"},{"location":"submit-resources/#spreadsheets","title":"Spreadsheets","text":"<p>This method is preferred, because the submitters can control which metadata values are exported and because format transformations by UMN Staff are not necessary. The GeoBTAA Metadata Template shows all of the fields needed for the Geoportal.</p>"},{"location":"submit-resources/#api-harvesting-or-html-parsing","title":"API Harvesting or HTML Parsing","text":"<p>Most data portals have APIs or HTML structures that can be programmatically parsed to obtain metadata for each record.</p> <code>DCAT enabled portals</code> <p>ArcGIS Open Data Portals (HUB), Socrata portals, and some others share metadata in the DCAT standard.</p> <code>CKAN / DKAN portals</code> <p>This application uses a custom metadata schema for their API.</p> <code>HTML Parsing</code> <p>If a data portal or website does not have an API, we may be able to parse the HTML pages to obtain the metadata needed to create GeoBlacklight schema records. </p> <code>OAI-PMH</code> <p>The Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) is a framework that can be used to harvest metadata records from enabled repositories. The records are usually available as a simple Dublin Core XML format. If the protocol is not set up to include extra fields, such as the map image's download link or bounding box, this method may not be sufficient on its own.</p>"},{"location":"submit-resources/#individual-metadata-files","title":"Individual Metadata files","text":"<p>Geospatial metadata standards are expressed in the XML or JSON format, which can be parsed to extract metadata needed to create GeoBlacklight schema records. Common standards for geospatial resources include:</p> <ul> <li>ISO 19139</li> <li>FGDC</li> <li>ArcGIS 1.0</li> <li>MARC</li> <li>MODS</li> </ul> <p>Tip</p> <p>The best way to transfer MARC records is to send a single file containing multiple records in the .MRC or MARC XML format. The Metadata Coordinator will use MarcEdit or XML parsing to transform the records.</p>"},{"location":"submit-resources/#3-contact-the-btaa-gin-product-manager","title":"3. Contact the BTAA-GIN Product Manager","text":"<p>Send an email, Slack message to the Product Manager / Metadata Coordinator.</p> <p>Minimum information to include:</p> <ul> <li>Title and Description of the collection</li> <li>a link to the website</li> <li>(If known) information about how to harvest the metadata or construct access links. </li> </ul> <p>The submission will be added to our collections processing queue.</p> <p>Info</p> <p>Metadata processing tasks are tracked on our public GitHub project dashboard.</p>"},{"location":"supplementalMetadata/","title":"Supplemental Metadata","text":"<p>All other forms of metadata, such as ISO 19139, FGDC Content Standard for Digital Geospatial Metadata, attribute table definitions, or custom schemas are treated as Supplemental Metadata.</p> <ul> <li>Supplemental Metadata is not usually edited directly for inclusion in the project.</li> <li>If this metadata is available as XML or HTML, it can be added as a hosted link for the Metadata preview tool in GeoBlacklight.</li> <li>XML or HTML files can be parsed to extract metadata that forms the basis for the item\u2019s GeoBlacklight schema record.</li> <li>The file formats that can be viewed within the geoportal application include:<ul> <li>ISO 19139 XML</li> <li>FGDC XML</li> <li>MODS XML</li> <li>HTML (any standard)</li> </ul> </li> </ul>"},{"location":"recipes/","title":"About","text":"<p>These recipes are step-by-step how-to guides for harvesting and processing metadata from the sites that we harvest the most frequently.</p> <p>The scripts needed for these recipes are all in the form of Jupyter Notebooks. To get started, download, fork, or clone the Harvesting Guide repository.</p> <p>Warning</p> <p>These recipes are not guaranteed to work! Since they rely on external websites, the scripts are necessarily works-in-progress. They need to be regularly updated and reconfigured in response to changes at the source website, python updates, and adjustments to our metadata schema.</p>"},{"location":"recipes/R-01_arcgis-hubs/","title":"ArcGIS","text":""},{"location":"recipes/R-01_arcgis-hubs/#purpose","title":"Purpose","text":"<p>To scan the DCAT 1.1 API of ArcGIS Hubs and return the metadata for all suitable items as a CSV file in the GeoBTAA Metadata Application Profile.</p> <p> </p> <p>This recipe includes steps that use the GBL Admin toolkit. Access to this tool is restricted to UMN BTAA-GIN staff and requires a login account. External users can create their own list or use one provided in this repository.</p> <pre><code>graph TB\n\nA{{STEP 1. &lt;br&gt;Download arcHubs.csv}}:::green --&gt; B[[STEP 2. &lt;br&gt;Run Jupyter Notebook harvest script]]:::green;\nB --&gt; C{Did the script run successfully?};\nC --&gt; |Yes| K[[STEP 3. Validate and Clean]]:::green; \nK --&gt; E[[STEP 4. &lt;br&gt;Upload the CSV]]:::green; \nE --&gt; L[[STEP 5. &lt;br&gt;Publish and republish records]]:::green; \nL --&gt; M[[STEP 6. &lt;br&gt;Unpublish retired records]]:::green; \nM --&gt; N{{STEP 7. &lt;br&gt;Record changes}}:::red; \nC --&gt; |No| D[Troubleshoot]:::yellow;\nD --&gt; H{Did the script stall because of a Hub?};\nH --&gt; |Yes| I[Refer to the page Update ArcGIS Hubs]:::yellow;\nH --&gt; |No &amp; I can't figure it out.| F{{Refer issue back to Product Manager}}:::red;\nH --&gt; |No| J[Try updating your Python modules or debugging the error message]:::yellow;\nJ --&gt; B;\nI --&gt; A;\n\n\nclassDef green fill:#E0FFE0\nclassDef yellow fill:#FAFAD2\nclassDef red fill:#E6C7C2\n\n\nclassDef questionCell fill:#fff,stroke:#333,stroke-width:2px;\nclass C,H questionCell;\n</code></pre>"},{"location":"recipes/R-01_arcgis-hubs/#step-1-download-the-list-of-active-arcgis-hubs","title":"Step 1: Download the list of active ArcGIS Hubs","text":"<p>We maintain a list of active ArcGIS Hub sites in GBL Admin. </p> <p>Shortcut</p> <p>Pre-formatted GBL Admin query link</p> <ol> <li>Go to the Admin (https://geo.btaa.org/admin) dashboard</li> <li>Filter for items with these parameters:<ul> <li>Resource Class: Websites</li> <li>Accrual Method: DCAT US 1.1</li> </ul> </li> <li>Select all the results and click Export -&gt; CSV</li> <li>Download the CSV and rename it <code>arcHubs.csv</code></li> </ol> <p>Info</p> <p>Exporting from GBL Admin will produce a CSV containing all of the metadata associated with each Hub. For this recipe, the only fields used are:</p> <ul> <li>ID: Unique code assigned to each portal. This is transferred to the \"Is Part Of\" field for each dataset.</li> <li>Title: The name of the Hub. This is transferred to the \"Provider\" field for each dataset</li> <li>Publisher: The place or administration associated with the portal. This is applied to the title in each dataset in brackets</li> <li>Spatial Coverage: A list of place names. These are transferred to the Spatial Coverage for each dataset</li> <li>Member Of: a larger collection level record. Most of the Hubs are either part of our Government Open Geospatial Data Collection or the Research Institutes Geospatial Data Collection</li> </ul> <p>However, it is not necessary to take extra time and manually remove the extra fields from the CSV export, because the Jupyter Notebook code will ignore them.</p>"},{"location":"recipes/R-01_arcgis-hubs/#step-2-run-the-harvest-script","title":"Step 2: Run the harvest script","text":"<ol> <li>Start Jupyter Notebook and navigate to the Recipes directory.</li> <li>Open R-01_arcgis-hubs.ipynb</li> <li>Move the downloaded file <code>arcHubs.csv</code> into the same directory as the Jupyter Notebook.</li> <li>Run all cells.</li> </ol>"},{"location":"recipes/R-01_arcgis-hubs/#troubleshooting-script-failures","title":"Troubleshooting script failures","text":"<p>Warning</p> <p>The Hub sites are fairly unstable and it is likely that one or more of them will occasionally fail and interrupt the script. </p> <p>If the script stalls or fails to parse certain Hub sites, try the following:</p> <ol> <li>Visit the URL for the Hub to check and see if the site is down, moved, etc. </li> <li>Refer to the page on How to remove broken or deprecated ArcGIS Hubs for more guidance on how to edit the website record.<ul> <li>If a site is missing: Unpublish it from GBL Admin, indicate the Date Retired, and make a note in the Status field.  </li> <li>If a site is still live, but the JSON API link is not working: remove the value \"DCAT US 1.1\" from the Accrual Method field and make a note in the Status field.</li> <li>If the site has moved to a new URL, update the website record with the new information.</li> </ul> </li> <li>Start over from Step 1.</li> </ol>"},{"location":"recipes/R-01_arcgis-hubs/#step-3-validate-and-clean","title":"Step 3: Validate and Clean","text":"<p>Although the harvest notebook will produce valide metadata for most of the items, there may still be some errors. Run the cleaning script to ensure that the records are valid before we try to ingest them into GBL Admin. 1. Move the scanned records csv into the cleaned-00 recipe folder 2. In Jupyter Notebook, navigate to the R-00 folder 3. Replace the name of the csv in the first cell in the notebook to reflect the current csv\u2019s filename 4. Run all cells. Takes less than a minute. </p>"},{"location":"recipes/R-01_arcgis-hubs/#step-4-upload-the-csv","title":"Step 4: Upload the CSV","text":"<p>Tip</p> <p>See also: General Import Instructions</p> <ol> <li>In GBL Admin, select Imports in the Admin Tools menu at the top of the page, then New Import.</li> <li>Enter the Name \"[GitbHub Issue Number] ArcGIS Hubs scan YYYY-MM-DD\" and the type \"BTAA CSV.\" </li> <li>Click Choose File and upload the cleaned scanned records of today\u2019s date (likely still located in the R-00_clean recipe folder). </li> <li>Click Create Import. Wait! The page may not immediately change. </li> <li> <p>Briefly review the Field Mappings to make sure none of the fields are blank. No changes should be needed. Click Create Mapping at the bottom of the page. </p> <p>Optional verification</p> <p>Check that the CSV Row Count matches the actual row count of your CSV. (In Excel, select any column header, then find the row count at the bottom of the window. It will be 1 greater than expected because it includes the column header row.) </p> </li> <li> <p>Click Run Import, then wait for about 30 minutes or more. Refresh the page to see how many records have been imported so far. The import is complete when the Total count under Imported Documents matches the CSV Row Count. There is no notification. </p> </li> </ol>"},{"location":"recipes/R-01_arcgis-hubs/#step-5-publish-and-republish-records","title":"Step 5: Publish and republish records","text":""},{"location":"recipes/R-01_arcgis-hubs/#convert-new-records-from-draft-to-published","title":"Convert new records from Draft to Published:","text":"<ol> <li>Click the GBL*Admin logo to return to the Documents view.</li> <li>Click Imports at the top of the left column menu and click on today\u2019s ArcGIS Hub import.</li> <li>Under publication state, click on Draft. </li> <li>Record the number of Draft records in a comment on the GitHub issue. This is the \u201cNew\u201d stat used in the last step. </li> <li>Check the box to select all, Select all results that match this search, select Bulk Actions &gt; Published, and Run Bulk Action. Reload the page to see the results.</li> </ol>"},{"location":"recipes/R-01_arcgis-hubs/#republish-previously-unpublished-records-that-have-returned","title":"Republish previously unpublished records that have returned:","text":"<ol> <li>Return to the Documents view, click on Imports, and select today\u2019s import.</li> <li>Under publication state, click on Unpublished. <ul> <li>These are items that were found in past scans, then not found and therefore removed in subsequent scans, but have now been found once again in today\u2019s scan. </li> </ul> </li> <li>In a comment on the GitHub issue, record the number of unpublished records. This is the \u201cRepublished\u201d stat used in the last step. </li> <li>Check the box to select all, Select all results that match this search, and select Bulk Actions &gt; Published, Run Bulk Action. Again, reload the page to see the results. </li> <li>Return to the Documents view, click on Imports, and select today\u2019s import.</li> <li>Under Publication State, confirm that \u201cpublished\u201d is now the only category and that the number of records matches the row count of your CSV upload. </li> <li>Click the red X next to the title of today\u2019s important so that all records are shown.</li> </ol>"},{"location":"recipes/R-01_arcgis-hubs/#step-6-unpublish-retired-records","title":"Step 6: Unpublish retired records","text":"<p>Purpose</p> <p>The purpose of this step is to unpublish from the Geoportal any records that have been removed from their source hub by their owners. When an existing record is found and imported in a new scan, its Date Accessioned is updated. We want to keep records found in the most recent two scans (including today's) but unpublish everything older than that. This should remove intentionally retired records but leave any that were just temporarily unavailable. </p> <ol> <li>If you're not there already, return to the Documents view so you're viewing all records. Scroll down to Accrual Method and click ArcGIS Hub. </li> <li>Click Published under Publication State to see all published Hub records.</li> <li>Under Date Accessioned, identify any results with a date earlier than the two most recent dates, including today's. (There might be only one or none). Note that they are sorted by number of records, not by date! </li> <li>In a comment on the GitHub issue, record the total number of records on those earlier dates as \u201cRetired.\u201d</li> <li>For each of these earlier dates: Select the date, check the box to select all records, click Select all results that match this search, select Bulk Actions &gt; Unpublished, and click Run Bulk Action. Again, reload the page to see the results. </li> </ol> <p>Tip</p> <p>If you see weird behavior during this step, scroll down to \u201cNotes and Troubleshooting\u201d</p>"},{"location":"recipes/R-01_arcgis-hubs/#step-7-record-what-has-changed","title":"Step 7: Record what has changed","text":"<ol> <li>Optional but helpful: save your GitHub comment with the numbers you recorded of new, republished, and retired records. </li> <li>Mandatory: In the right sidebar, expand Collections under Projects. Put the sum of new + republished in the Records Added field, and fill in the Records Retired field. </li> </ol> <p>Info</p> <p>This information gets exported and added to a monthly report to showcase the way our collection fluctuates. As a general rule of thumb, the total change in records shouldn\u2019t be much more than 100 - if it\u2019s a lot more, try to evaluate why or ask for help. </p>"},{"location":"recipes/R-01_arcgis-hubs/#note-on-records-that-do-not-respond-to-edits","title":"Note on records that do not respond to edits","text":"<ul> <li>Sometimes GBL Admin records will appear in the list that you get when you click \u201cpublished\u201d under publication state and then click the older date accessioned saved queries, but when viewed in the results list, have a red \u201cunpublished\u201d label. (this mismatch can also happen in reverse - if something\u2019s unpublished but you can\u2019t publish it, or if there are any other records are not updated and syncing)</li> <li>The record lives in 2 places: admin (database - back end) and geoportal (solr - front end). If it\u2019s corrupted, it \u201cfreezes\u201d in the front end. No changes you make to it will apply, and it may display incorrect values. This is almost always because of the bounding box, but it can also be because of the date range. To resolve:<ol> <li>Click on the record title</li> <li>Click under \u201cadmin view\u201d </li> <li>Scroll down to the Spatial section and either clear or fix the Bounding Box and Geometry fields.<ul> <li>If you don't know the correct values, it's fine to leave both fields blank.</li> <li>If you're going to add corrected values, it's fine to just fill in the Bounding Box field. When you click save, it will automatically generate the geometry based on the bounding box.</li> <li>If you know the specific geometry that should applied, you can add it, but it's not important. </li> </ul> </li> </ol> </li> </ul>"},{"location":"recipes/R-02_socrata/","title":"Socrata","text":""},{"location":"recipes/R-02_socrata/#purpose","title":"Purpose","text":"<p>To scan the DCAT API of Socrata Data Portals and return the metadata for all suitable items as a CSV file in the GeoBTAA Metadata Application Profile.</p> <p>Note: This recipe is very similar to the ArcGIS Hubs Scanner.</p> <p> </p> <p>This recipe includes steps that use the metadata toolkit GBL Admin. Access to GBL Admin is restricted to UMN BTAA-GIN staff and requires a login account. External users can create their own list or use one provided in this repository.</p> <pre><code>graph TB\n\nA{{STEP 1. &lt;br&gt;Download socrataPortals.csv}}:::green --&gt; B[[STEP 2. &lt;br&gt;Run Jupyter Notebook harvest script]]:::green;\nB --&gt; C{Did the script run successfully?}:::white;\nC --&gt; |No| D[Troubleshoot]:::yellow;\nD --&gt; H{Did the script stall because of a portal?}:::white;\nH --&gt; |Yes| I[Remove or update the portal from the list]:::yellow;\nH --&gt; |No &amp; I can't figure it out.| F[Refer issue back to Product Manager]:::red;\nH --&gt; |No| J[Try updating your Python modules or investigating the error]:::yellow;\nJ --&gt; B;\nI --&gt; A;\nC --&gt; |Yes| K[[STEP 3. Validate and Clean]]:::green; \nK --&gt; E[STEP 4. &lt;br&gt;Publish/unpublish records in GBL Admin]:::green; \n\nclassDef green fill:#E0FFE0\nclassDef yellow fill:#FAFAD2\nclassDef red fill:#E6C7C2\nclassDef white fill:#FFFFFF\n</code></pre>"},{"location":"recipes/R-02_socrata/#step-download-the-list-of-active-socrata-data-portals","title":"Step: Download the list of active Socrata Data Portals","text":"<p>We maintain a list of active Socrata Hub sites in GBL Admin. </p> <p>Shortcut</p> <p>Pre-formatted GBL Admin query link</p> <ol> <li>Go to the GBL Admin dashboard</li> <li>Use the Advanced Search to filter for items with these parameters:<ul> <li>Format: \"Socrata data portal\"</li> </ul> </li> <li>Select all the results and click Export -&gt; CSV</li> <li>Download the CSV and rename it <code>socrataPortals.csv</code></li> </ol> <p>Info</p> <p>Exporting from GBL Admin will produce a CSV containing all of the metadata associated with each Hub. For this recipe, the only fields used are:</p> <ul> <li>ID: Unique code assigned to each portal. This is transferred to the \"Is Part Of\" field for each dataset.</li> <li>Title: The name of the Hub. This is transferred to the \"Provider\" field for each dataset</li> <li>Publisher: The place or administration associated with the portal. This is applied to the title in each dataset in brackets</li> <li>Spatial Coverage: A list of place names. These are transferred to the Spatial Coverage for each dataset</li> <li>Member Of: a larger collection level record. Most of the Hubs are either part of our Government Open Geospatial Data Collection or the Research Institutes Geospatial Data Collection</li> </ul> <p>It is not necessary to take extra time and manually remove the unused fields, because the Jupyter Notebook code will ignore them.</p>"},{"location":"recipes/R-02_socrata/#step-2-run-the-harvest-script","title":"Step 2: Run the harvest script","text":"<ol> <li>Start Jupyter Notebook and navigate to the Recipes directory.</li> <li>Open R-02_socrata.ipynb</li> <li>Move the downloaded file <code>socrataPortals.csv</code> into the same directory as the Jupyter Notebook.</li> </ol>"},{"location":"recipes/R-02_socrata/#troubleshooting-as-needed","title":"Troubleshooting (as needed)","text":"<ol> <li>Visit the URL for the Socrata Portal to check and see if the site is down, moved, etc. </li> <li>If a site is missing<ul> <li>Unpublish it from GBL Admin and indicate the Date Retired, and make a note in the Status field.  </li> </ul> </li> <li>Start over from Step 1.</li> </ol>"},{"location":"recipes/R-02_socrata/#step-3-validate-and-clean","title":"Step 3: Validate and Clean","text":"<p>Although the harvest notebook will produce valide metadata for most of the items, there may still be some errors. Run the cleaning script to ensure that the records are valid before we try to ingest them into GBL Admin.</p>"},{"location":"recipes/R-02_socrata/#step-4-upload-to-gbl-admin","title":"Step 4: Upload to GBL Admin","text":"<ol> <li>Review the previous upload. Check the Date Accessioned field of the last harvest and copy it. </li> <li>Upload the new CSV file. This will overwrite the Date Accessioned value for any items that were already present.</li> <li>Use the old Date Accessioned value to search for the previous harvest date. </li> <li>Unpublish the ones that have the old date in the Date Accessioned field 5. Record this number in the GitHub issue for the scan under Number Deleted</li> <li>Look for records in the uploaded batch that are still \"Draft\" - these are new records. </li> <li>Publish them and record this number in the GitHub issue under Number Added</li> </ol>"},{"location":"recipes/R-03_ckan/","title":"CKAN","text":""},{"location":"recipes/R-03_ckan/#purpose","title":"Purpose","text":"<p>To scan the Action API for CKAN data portals and retrieve metadata for new items while returning a list of deleted items.</p> <p>Warning</p> <p>This batch CKAN recipe is being deprecated and replaced with recipes tailored to each site.</p> <pre><code>graph TB\n\nA((STEP 1. &lt;br&gt;Set up directories)) --&gt; B[STEP 2. &lt;br&gt;Run Jupyter Notebook script] ;\nB --&gt; C{Did the script run successfully?};\nC --&gt; |No| D[Troubleshoot];\nD --&gt;A;\nC --&gt; |No &amp; I can't figure it out.| F[Refer issue back to Product Manager];\nC --&gt; |Yes| E[STEP 3. &lt;br&gt;Edit places names &amp; titles]; \nE --&gt; G[STEP 4. &lt;br&gt;Upload new records];\nG --&gt; H[STEP 5. &lt;br&gt;Unpublish deleted records];\n\nclassDef goCell fill:#99d594,stroke:#333,stroke-width:2px\nclass A,B,C,E,G goCell;\nclassDef troubleCell fill:#ffffbf,stroke:#333,stroke-width:2px;\nclass D troubleCell;\nclassDef endCell fill:#fc8d59,stroke:#333,stroke-width:2px\nclass F,H endCell;\nclassDef questionCell fill:#fff,stroke:#333,stroke-width:2px;\nclass C questionCell;\n\n\n</code></pre>"},{"location":"recipes/R-03_ckan/#step-1-set-up-your-directories","title":"Step 1: Set up your directories","text":"<ol> <li> <p>Navigate to your local Recipes directory for R-03_ckan.</p> </li> <li> <p>Verify that there are two folders</p> <ul> <li><code>resource</code>: contains a CSV for each portal per harvest that lists all of the dataset identifiers</li> <li><code>reports</code>: combined CSV metadata files for all new and deleted datasets per harvest</li> </ul> </li> <li> <p>Review the CKANportals.csv file. Each active portal should have values in the following fields:</p> <ul> <li>portalName</li> <li>URL</li> <li>Provider</li> <li>Publisher</li> <li>Spatial Coverage</li> <li>Bounding Box </li> </ul> </li> </ol>"},{"location":"recipes/R-03_ckan/#step-2-run-the-harvest-script","title":"Step 2: Run the harvest script","text":"<ol> <li>Start Jupyter Notebook </li> <li>Open your local copy of R-03_ckan.ipynb</li> </ol> <p>Info</p> <p>This script will harvest from a set of CKAN data portals. It saves a list of datasets found in each portal and will compare the output between runs. The result will be two CSVs: new items and deleted items.</p> <p>The script only harvests items that can be identified as shapefiles or imagery.</p>"},{"location":"recipes/R-03_ckan/#step-3-edit-the-metadata-for-new-items","title":"Step 3: Edit the metadata for new items","text":"<p>The new records can be found in <code>reports/allNewItems_{today's date}.csv</code> and will need some manual editing. </p> <ul> <li>Spatial Coverage: Add place names related to the datasets.</li> <li>Title: Concatenate values in the Alternative Title column with the Spatial Coverage of the dataset. </li> </ul>"},{"location":"recipes/R-03_ckan/#step-4-upload-metadata-for-new-records","title":"Step 4: Upload metadata for new records","text":"<p>Open GBL Admin and upload the new items found in <code>reports/allNewItems_{today's date}.csv</code></p>"},{"location":"recipes/R-03_ckan/#step-5-delete-metadata-for-retired-records","title":"Step 5: Delete metadata for retired records","text":"<p>Unpublish records found in <code>reports/allDeletedItems_{today's date}.csv</code>. This can be done in GBL Admin manually (one by one) or with the GBL Admin documents update script.</p>"},{"location":"recipes/R-04_oai-pmh/","title":"Harvest via OAI-PMH","text":"<p>Using Illinois Library Digital Collections as example</p> <p>Steps:</p>"},{"location":"recipes/R-04_oai-pmh/#part-1-get-the-files-via-oai","title":"Part 1: get the files via oai","text":"<ol> <li>Use this OAI-PMH validator tool at https://validator.oaipmh.com</li> <li>Go to the Download XML tab</li> <li>Enter the base URL (https://digital.library.illinois.edu/oai-pmh) and the set name (6ff64b00-072d-0130-c5bb-0019b9e633c5-2)</li> <li>Wait for the app to pull all the XML files and download them (ideally in a ZIP, but sometimes that doesn't work and you need to click on each file)</li> </ol>"},{"location":"recipes/R-04_oai-pmh/#part-2-turn-the-records-into-a-csv-via-openrefine","title":"Part 2: turn the records into a CSV via OpenRefine","text":"<ol> <li>start OpenRefine</li> <li>Choose \"Get Data from this Computer\" and upload the XML files</li> <li>From the parsing options, select from the Header \"record\"</li> </ol>"},{"location":"recipes/R-04_oai-pmh/#part-3-collapse-multivalued-cells","title":"Part 3: Collapse multivalued cells","text":"<ol> <li>The multi-valued cells will start out being grouped together by which XML file they came from. We don't want that, so remove the column called File.</li> <li>Now, they are grouped by a value \"http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd\" Leave this for now.</li> <li>There are multiple Identifiers (dc:identifier), so select that column, Edit Cells - Join multi-valued cells</li> <li>Move the Identifier column to the beginning so that items will be grouped by these unique values</li> <li>Collapse the remaining cells with the same Join Multi-valued cells function</li> <li>Export to CSV</li> </ol>"},{"location":"recipes/R-05_iiif/","title":"IIIF","text":""},{"location":"recipes/R-05_iiif/#purpose","title":"Purpose","text":"<p>To extract metadata from IIIF JSON Manifests.</p>"},{"location":"recipes/R-05_iiif/#step-1-download-the-jsons","title":"Step 1: Download the JSONs","text":"<ol> <li>Create a CSV file called \"jsonUrls.csv\" with just the URLs of the JSONs.</li> <li>Navigate to the Terminal/Command Line and into a directory where you can save files</li> <li>Type: <code>wget -i jsonUrls.csv</code></li> <li>Review that all of the JSONs downloaded to a local directory</li> </ol>"},{"location":"recipes/R-05_iiif/#step-2-run-the-extraction-script","title":"Step 2: Run the extraction script","text":"<ol> <li>Start Jupyter Notebook and navigate to the Recipes directory.</li> <li>Open R-05_iiif.ipynb</li> <li>Move the downloaded file <code>jsonUrls.csv</code> into the same directory as the Jupyter Notebook.</li> <li>Run all cells</li> </ol> <p>Warning</p> <p>This will lump all the subsections into single fields and the user will still need to split them.</p>"},{"location":"recipes/R-05_iiif/#step-3-merge-the-metadata","title":"Step 3: Merge the metadata","text":"<p>Although the Jupyter Notebook extracts the metadata to a flat CSV, we still need to merge this with any existing metadata for the records.</p>"},{"location":"recipes/R-05_iiif/#step-4-upload-to-gbl-admin","title":"Step 4: Upload to GBL Admin","text":""},{"location":"recipes/R-06_mods/","title":"MODS","text":""},{"location":"recipes/R-06_mods/#purpose","title":"Purpose:","text":"<p>To extract metadata from XML files in the MODS metadata format. </p>"},{"location":"recipes/R-06_mods/#step-1-obtain-a-list-of-urls-for-the-xml-files","title":"Step 1: Obtain a list of URLs for the XML files","text":"<p>This list may be supplied by the submitter or we may need to query the website to find them.</p>"},{"location":"recipes/R-06_mods/#step-2-run-the-extraction-script","title":"Step 2: Run the extraction script","text":"<ol> <li>Start Jupyter Notebook and navigate to the Recipes directory.</li> <li>Open R-06-mods.ipynb</li> <li>Move the downloaded file <code>arcHubs.csv</code> into the same directory as the Jupyter Notebook.</li> <li>Run all cells.</li> </ol>"},{"location":"recipes/R-06_mods/#step-3-format-as-opengeometadata","title":"Step 3: Format as OpenGeoMetadata","text":"<p>Manually adjust the names of the columns to match metadata into our GeoBTAA metadata template.</p>"},{"location":"recipes/R-06_mods/#step-4-upload-to-gbl-admin","title":"Step 4: Upload to GBL Admin","text":""},{"location":"recipes/R-07_ogm/","title":"OpenGeoMetadata","text":""},{"location":"recipes/R-07_ogm/#purpose","title":"Purpose","text":"<p>To convert OpenGeoMetadata (GeoBlacklight) JSONs to a CSV in the GeoBTAA Metadata Profile</p>"},{"location":"recipes/R-07_ogm/#step-1-obtain-the-jsons","title":"Step 1: Obtain the JSONs","text":"<p>Collect the metadata JSONs. This type of metadata typically is obtain in one of the following ways:</p> <ul> <li>Direct submission (Team Members from Wisconsin)</li> <li>Via an OpenGeoMetadata repository</li> </ul>"},{"location":"recipes/R-07_ogm/#step-2-run-the-conversion-script","title":"Step 2: Run the conversion script","text":"<ol> <li>Start Jupyter Notebook and navigate to the Recipes directory.</li> <li>Open R-07_openGeoMetadata.ipynb</li> <li>Move the folder with the metadata JSONs into the same directory as the Jupyter Notebook.</li> <li>Declare the paths and folder name in the Notebook.</li> <li>Run all cells</li> </ol> <p>Tip</p> <p>Depending upon the source, you may want to adjust the script to accomodate custom fields.</p>"},{"location":"recipes/R-07_ogm/#step-3-edit-the-output-csv","title":"Step 3: Edit the output CSV","text":"<p>The GeoBTAA Metadata Profile may have additional or different requirements. Consult with the Product Manager on which fields may need augmentation.</p>"},{"location":"recipes/R-07_ogm/#step-4-upload-to-gbl-admin","title":"Step 4: Upload to GBL Admin","text":""},{"location":"recipes/R-08_pasda/","title":"PASDA","text":""},{"location":"recipes/R-08_pasda/#purpose","title":"Purpose","text":"<p>To harvest metadata from Pennsylvania Spatial Data Access (PASDA), a custom data portal. To begin, start the Jupyter Notebook:</p> <ol> <li>Start Jupyter Notebook and navigate to the Recipes directory.</li> <li>Open R-08_pasda.ipynb</li> </ol>"},{"location":"recipes/R-08_pasda/#step-1-obtain-a-list-of-landing-pages","title":"Step 1: Obtain a list of landing pages","text":"<p>Run Part 1 of the Notebook to obtain a list of all of the records currently in PASDA. This list can be found by doing a blank search on the PASDA website: (https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+)</p> <p>Since this is a large result list, the recipe recommends downloading the HTML file to your desktop (In the Safari browser, this is File-Save As - Save As Page Source)</p> <p>Then, we can use the Beautiful Soup module to query this page and harvest the following values:</p> <ul> <li>Title</li> <li>Date Issued</li> <li>Publisher</li> <li>Description</li> <li>Metadata file link</li> <li>Download link</li> </ul>"},{"location":"recipes/R-08_pasda/#step-2-download-the-supplemental-metadata-files","title":"Step 2: Download the supplemental metadata files","text":"<p>Context</p> <p>Bounding boxes &amp; keywords are not found in the landing pages, but most of the PASDA datasets have a supplemental metadata document, which does contain coordinates. The link to this document was scraped to the 'Metadata File\" column during the previous step.</p> <p>Most of the records have supplemental metadata in ISO 19139 or FGDC format. The link to this document is found in the 'Metadata File\" column. Although these files are created as XMLs, the link is a rendered HTML.</p> <p>There is additional information in these files that we want to scrape, including bounding boxes and geometry type.</p> <p>At the end of Step 2, you will have a folder of HTML metadata files.</p>"},{"location":"recipes/R-08_pasda/#step-3-query-the-downloaded-supplemental-metadata-files","title":"Step 3: Query the downloaded supplemental metadata files","text":"<p>This section of the script will scan each HTML metadata file. If it contains bounding box information, it will pull the coordinates. Otherwise, it will assign a default value of the State of Pennsylvania extents.</p> <p>It will also pull the geometry type and keywords, if available.</p>"},{"location":"recipes/R-08_pasda/#step-4-add-default-and-calculated-values","title":"Step 4: Add default and calculated values","text":"<p>This step will clean up the harvested metadata and add our administrative values to each row.  At the end, there will be a CSV file in your directory named for today's date.</p>"},{"location":"recipes/R-08_pasda/#step-5-upload-the-csv-to-gbl-admin","title":"Step 5: Upload the CSV to GBL Admin","text":"<ol> <li>Upload the new records to GBL Admin</li> <li>Use the Date Accessioned field to search for records that were not present in the current harvest. Retire any records that have the code \"08a-01\" but were not part of this harvest.</li> </ol>"},{"location":"recipes/R-09_umedia/","title":"UMedia","text":""},{"location":"recipes/R-09_umedia/#purpose","title":"Purpose","text":"<p>To harvest new records added to the University Of Minnesota's UMedia Digital Library.</p>"},{"location":"recipes/R-09_umedia/#step-1-set-up-folders","title":"Step 1: Set up folders","text":"<ol> <li>Navigate to the UMedia Recipe directory at R-09_umedia.ipynb</li> <li>Verify the following folders are present:</li> </ol> <p><code>requests</code></p> <p>This folder stores all search results in JSON format for each reaccession as <code>request_YYYYMMDD.json</code>. </p> <p><code>jsons</code></p> <p>This folder stores all JSON files by different added month for UMedia maps. After we get the search result JSON file from each reaccession, we will read this <code>request_YYYYMMDD.json</code> file in detail to filter out the included maps by month, and store them to <code>dateAdded_YYYYMM.json</code> individually.</p> <p><code>reports</code></p> <p>This folder stores all CSV files for metadata by month. Once we have JSON files for different month, we extract all useful metadata and contribute in the <code>dateAdded_YYYYMM.csv</code> in this folder.</p>"},{"location":"recipes/R-09_umedia/#step-2-run-the-harvesting-script","title":"Step 2: Run the harvesting script","text":"<ol> <li>Start Jupyter Notebook and open R-09_umedia.ipynb</li> <li>The second code cell will ask for an input on how many map records you want to harvest.</li> <li>The third code cell will ask for a date range. Select a month (in the form <code>yyyy-mm</code>)  based on the last time you ran the script.</li> </ol>"},{"location":"recipes/R-09_umedia/#step-3-edit-the-metadata","title":"Step 3: Edit the metadata","text":""},{"location":"recipes/R-09_umedia/#step-4-upload-to-gbl-admin","title":"Step 4: Upload to GBL Admin","text":""},{"location":"recipes/add-bbox/","title":"Add bounding boxes","text":"<p>Summary</p> <p>This page describes processes for obtaining bounding box coordinates for our scanned maps. The coordinates will be used for indexing the records in the Big Ten Academic Alliance Geoportal.</p> <p>**About bounding box coordinates for the BTAA Geoportal **</p> <ul> <li>Bounding boxes enable users to search for items with a map interface. </li> <li>The format is 4 coordinates in decimal degrees </li> <li>Provide the coordinates in this order: West, South, East, North. </li> <li>The bounding boxes do not need to be exact, particularly with old maps that may not be very precise anyways.</li> </ul>"},{"location":"recipes/add-bbox/#manual-method","title":"Manual method","text":""},{"location":"recipes/add-bbox/#part-a-setup","title":"Part A: Setup","text":"<ol> <li>Open and inspect the image file.</li> <li>Try to identify a single / combined region that the map or atlas represents</li> <li>You can also check to see if the map has the bounding coordinates printed in the text anywhere or you are able to find the bounds by inspecting the edges.  </li> <li>Open another window with the Klokan Bounding Box tool.</li> <li>Set the Copy &amp; Paste section to CSV.</li> </ol>"},{"location":"recipes/add-bbox/#part-b-find-the-coordinates","title":"Part B: Find the coordinates","text":""},{"location":"recipes/add-bbox/#option-1-search-for-a-place-name","title":"Option 1: Search for a place name","text":"<ol> <li>Use the search boxes on the Klokan Bounding Box tool to zoom to the region.  (For example, search for \u201cIllinois\u201d.</li> <li>Manually adjust the grey overlay box in the Klokan site to line up the edges to the edges of the map. </li> <li>Try to line it up reasonably closely</li> </ol>"},{"location":"recipes/add-bbox/#option-2-draw-a-shape","title":"Option 2: Draw a shape","text":"<ol> <li>Switch to the Polygon tool by clicking on the pentagon icon</li> <li>Click as many points on the screen as needed to approximate the map extent.</li> <li>Click on the first point to close the polygon</li> <li>The interface will display a dotted line showing the bounding box rectangle.</li> </ol>"},{"location":"recipes/add-bbox/#part-c-copy-back-to-geobtaa-metadata","title":"Part C: Copy back to GeoBTAA metadata","text":"<ol> <li>Click the \u201cCopy to Clipboard\u201d icon on the Klokan site.</li> <li>Paste the coordinates into the Bounding Box field in the GeoBTAA metadata template or in the GBL Admin metadata editor.</li> </ol>"},{"location":"recipes/add-bbox/#programmatic-method","title":"Programmatic method","text":"<p>The OpenStreetMap offers and API that allows users to query with place names and return a bounding box. Follow the Tutorial, Use OpenStreetMap to generate bounding boxes, for this method.</p>"},{"location":"recipes/clean/","title":"Validate and Clean Metadata","text":"<p>Info</p> <p>Find the cleaning script here: https://github.com/geobtaa/harvesting-guide/tree/main/recipes/R-00_clean</p> <p>As a final step of Edit stage of the resource lifecycle, we run a cleaning script to fix common errors:</p>"},{"location":"recipes/clean/#required-fields","title":"Required fields","text":"<ul> <li>Resource Class: Checks that an entry exists and that it is one of the controlled values. If the field is empty, the cleaning script will insert <code>Datasets</code> as a default.</li> <li>Access Rights: Checks that it contains either <code>Public</code> or <code>Restricted</code>. If empty, the script will insert <code>Public</code> as a default.</li> </ul>"},{"location":"recipes/clean/#conditionally-required-fields","title":"Conditionally required fields","text":"<ul> <li>Format: If the \"Download\" field has a value, Format must also be present. If empty, the script will insert <code>File</code> as a default.</li> </ul>"},{"location":"recipes/clean/#syntax","title":"Syntax","text":"<ul> <li>Date Range: If present, checks that it is valid with a range in the format <code>yyyy-yyyy</code>, where the second value is earlier than the first. If the second value is earlier (lower) than the first, the script will reorder them.</li> <li>Bounding Box: There are numerous possible conditions that the script will fix:<ul> <li>Rounding: The script will round all coordinates to two decimal points. (For some collections, we change this to three). This is done because the bounding boxes function as a finding aid and overly precise coordinates can be misleading. It also saves on the memory load for geometry and centroid calculations.</li> <li>non-degrees: If one of the coordinates exceeds the earth's coordinates (over 180 for longitude or 90 for latitude), the coordinates are considered invalid and the entire field will be cleared for that record.</li> <li>lines or points: If the script finds that easting and westing or north and south coordinates are equal, it will add a .001 to the end.</li> </ul> </li> </ul>"},{"location":"recipes/clean/#reports","title":"Reports","text":"<p>After cleaning, the script will produce two CSVS:</p> <ol> <li>Cleaned Metadata: All of the original rows with fixes applied. A new column called \"Cleaned\" will indicate if the row was edited by the script.</li> <li>Cleaning Log: A list of all the records and fields that were cleaned and what was done.</li> </ol>"},{"location":"recipes/secondary-tables/","title":"How to upload links in secondary tables in GBL Admin","text":"<ul> <li>We use two compound metadata fields, <code>Multiple Download Links</code> and <code>Institutional Access Links</code>, that include multiple links that are formatted with both a label + a link. </li> <li>Because these fields are not regular JSON flat key:value pairs, they are stored in secondary tables within GBL Admin.<ul> <li>When using GBL Admin's Form view, these values can be entered by clicking into a side page linked from the record.</li> <li>For CSV uploads, these values are uploaded with a separate CSV from the one used for the main import template.</li> </ul> </li> </ul> <p>Tip</p> <p>See the opengeometadata.org page on multiple downloads for how these fields are formatted in JSON</p>"},{"location":"recipes/secondary-tables/#manual-entry","title":"Manual entry","text":""},{"location":"recipes/secondary-tables/#multiple-download-links","title":"Multiple Download Links","text":"<ol> <li>On the Form view, scroll down to the end of the Links section  and click the text \"Multiple Download Links\"</li> <li>Click the New Download URL button</li> <li>Enter a label (i.e., \"Shapefile\") and the download URL</li> <li>Repeat for as many as needed</li> </ol>"},{"location":"recipes/secondary-tables/#institutional-access-links","title":"Institutional Access Links","text":"<ol> <li>On the Form view, scroll down to the bottom of the right-hand navigation and click the text \"Institutional Access Links\" </li> <li>Click the New Access URL button</li> <li>Select an institution code and the access URL</li> <li>Repeat for as many as needed</li> </ol>"},{"location":"recipes/secondary-tables/#csv-upload-for-either-type","title":"CSV Upload for either type","text":"<ol> <li>Go to Admin Tools - Multiple Downloads or Access Links</li> <li>Upload a CSV in on of these formats:</li> </ol> <p>CSV field headers for secondary tables</p> Multiple DownloadsInstitutional Access Links <pre><code>    | friendlier_id       | label            | value      |\n    |---------------------|------------------|------------|\n    | ID of target record |  any string      | the link   |\n</code></pre> <pre><code>    | friendlier_id       | institution_code | access_URL |\n    |---------------------|------------------|------------|\n    | ID of target record |  2 digit code    | the link   |\n</code></pre>"},{"location":"recipes/split-bbox/","title":"Split Bounding Boxes that cross the 180th Meridian","text":""},{"location":"recipes/split-bbox/#problem","title":"Problem","text":"<p>The BTAA Geoportal does not display bounding boxes that cross the 180th meridian (also known as the International Date Line.) In these circumstances, the West coordinate will be a positive number, but the East coordinate will be negative.</p>"},{"location":"recipes/split-bbox/#solution","title":"Solution","text":"<p>One way to mitigate this is to create two bounding boxes for the OGM Aardvark <code>Geometry</code> field. The Bounding Box value will be the same, but the Geometry field will have a multipolygon that is made up of two adjacent boxes.</p> <p>The following script will scan a CSV of the records, identify which cross the 180th Meridian, and insert a multipolygon into a new column. </p> <p>The script was designed with the assumption that the input CSV will be in the OGM Aardvark format, likely exported from GBL Admin. The CSV file must contain a field for <code>Bounding Box</code>.  It may contain a <code>Geometry</code> field with some values that we do not want to overwrite.</p> <p>This script will create a new field called \"Bounding Box (WKT)\". Items that crossed the 180th Meridian will have a multipolygon in that field. Items that don't cross will not have a value in that field. Copy and paste only the new values into the <code>Geometry</code> column and reupload the CSV to GBL Admin.</p> <pre><code>import csv\n\ndef split_coordinates(coordinate_str):\n    if not coordinate_str:\n        return ''\n\n    coordinates = coordinate_str.split(',')\n\n    west, south, east, north = map(float, coordinates)\n\n    if west &gt; 0 and east &lt; 0:\n        polygon1 = f'({west} {south}, {179.9} {south}, {179.9} {north}, {west} {north}, {west} {south})'\n        polygon2 = f'({-179.9} {south}, {-179.9} {north}, {east} {north}, {east} {south}, {-179.9} {south})'\n        return f'MULTIPOLYGON(({polygon1}), ({polygon2}))'\n\n    return coordinate_str\n\n# Specify the input CSV file path\ninput_file = 'your_input_file.csv'\n\n# Specify the output CSV file path\noutput_file = 'your_output_file.csv'\n\n# Specify the name of the column with the Bounding Box coordinates\ncoordinate_column = 'Bounding Box'\n\n# Specify the name of the new column to store the updated coordinates\nnew_coordinate_column = 'New Bounding Box (WKT)'\n\n# Read the input CSV and process the coordinates\nwith open(input_file, 'r') as file:\n    reader = csv.DictReader(file)\n    fieldnames = reader.fieldnames + [new_coordinate_column]\n\n    with open(output_file, 'w', newline='') as output:\n        writer = csv.DictWriter(output, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for row in reader:\n            bounding_box = row[coordinate_column]\n            new_bounding_box = split_coordinates(bounding_box)\n            row[new_coordinate_column] = new_bounding_box\n            writer.writerow(row)\n\nprint(\"CSV processing completed!\")\n</code></pre>"},{"location":"recipes/standardize-creators/","title":"Best Practices for Standardizing Creator Field Data","text":"<p> Authors: Creator Standardization Working Group</p> <p>Date: 28 November 2022</p> <p>Info</p> <p>See this Journal Article for a more thorough description of this process:</p> <p>Laura Kane McElfresh (2023) Creator Name Standardization Using Faceted Vocabularies in the BTAA Geoportal, Cataloging &amp; Classification Quarterly, DOI: 10.1080/01639374.2023.2200430</p> <p>Creator names are a critical access point for the discovery of geospatial information. Within the BTAA Geoportal, creator names\u2013whether names of persons or corporate bodies\u2013are displayed on landing pages and the citation widget, and are indexed and faceted for searching and browsing. Standardizing the names of resource creators makes search results more predictable, thereby producing a better experience for Geoportal users.</p> <p>To ensure that the Geoportal\u2019s collocation functions operate properly, this document recommends using the formulation of personal and corporate body names as they are found in identity registries and, when creators are not available in those registries, provides guidance for formulating names of creators. We seek to provide consistency of creator names within our database through the recommendations provided below. This document does not address the manual creation or editing of identity registry records.</p> <p>These best practices assume that standardization of names will occur after data is ingested into the BTAA Geoportal; however this document may be used to inform description choices made before records are ingested into the Geoportal.</p>"},{"location":"recipes/standardize-creators/#preferred-identity-registries","title":"Preferred Identity Registries","text":"<p>These best practices recommend consulting one or two name registries when deciding how to standardize names of creators: the Faceted Application of Subject Terminology (FAST) or the Library of Congress Name Authority File (LCNAF). FAST is a controlled vocabulary based on the Library of Congress Subject Headings (LCSH) that is well-suited to the faceted navigation of the Geoportal. The LCNAF is an authoritative list of names, events, geographic locations and organizations used by libraries and other organizations to collocate authorized creator names to make searching and browsing easier.</p>"},{"location":"recipes/standardize-creators/#overview-of-the-process","title":"Overview of the Process","text":"<p>These best practices present the following workflow for standardizing names of creators:</p>"},{"location":"recipes/standardize-creators/#search-fast-for-the-creators-name","title":"Search FAST for the creator\u2019s name","text":"<ul> <li>If the creator\u2019s name is found, then use the name as found in FAST</li> <li>If there\u2019s no match in FAST, then consult the Guidance for Formulating Creator Names Not Present in the Registries Noted Above</li> </ul>"},{"location":"recipes/standardize-creators/#searching-fast","title":"Searching FAST","text":"<p>To search the FAST registry for a creator name, you may use either assignFAST or searchFAST. assignFAST is ideal for quick searches, while searchFAST allows for advanced searching.</p>"},{"location":"recipes/standardize-creators/#assignfast","title":"assignFAST","text":"<ul> <li>Go to http://experimental.worldcat.org/fast/assignfast/ and begin typing the creator name into the text box.</li> <li>assignFAST will suggest headings in FAST format. When the correct heading appears, click it in the list of suggestions. </li> <li>The selected heading will appear in the text box, highlighted for copying.<ul> <li>For example, if you type in <code>St. Francis, Minnesota</code>, you will see suggestions including <ul> <li>\u201cMinnesota--St. Francis (Anoka County) USE Minnesota--Saint Francis (Anoka County)\u201d</li> <li>\u201cMinnesota--St. Francis (Anoka Co.) USE Minnesota--Saint Francis (Anoka County)\u201d.</li> </ul> </li> <li>Click on either of those suggestions and you will receive the authorized form of the name: <code>Minnesota--Saint Francis (Anoka County)</code>.</li> </ul> </li> <li>Copy the authorized name from the text box and paste it into the spreadsheet.</li> </ul>"},{"location":"recipes/standardize-creators/#searching-lcnaf","title":"Searching LCNAF","text":"<p>When a name is not found in FAST, search the Library of Congress Name Authority File (LCNAF) for a match using the directions found in the Searching LCNAF section below. </p> <p>If no match is found, there\u2019s no requirement to do intensive research. Continue to the next section, Guidance for Formulating Creator Names Not Present in the Registries Noted Above. If using these Best Practices in a metadata sprint, you may alternatively move onto the next name in the sprint spreadsheet.</p>"},{"location":"recipes/standardize-creators/#guidance-for-formulating-creator-names-not-present-in-the-registries-noted-above","title":"Guidance for Formulating Creator Names Not Present in the Registries Noted Above","text":"<p>When a personal or corporate body name cannot be found in neither FAST nor LCNAF, follow the directions below.</p>"},{"location":"recipes/standardize-creators/#personal-names","title":"Personal Names","text":"<p>Personal names should be formulated in inverted order (last name first) based on the information that appears on the item in the Geoportal.</p> <pre><code>    Felsted, L. E.\n    Ackley, Seth\n    Colvert, DeLynn C.\n    Griffey, Ken, Jr. \n</code></pre> <p>In cases where extra information is needed to distinguish a name, you may add a parenthetical at the end of the name, e.g., Surveyor, Cartographer, Draftsman, Geologist, Engraver. </p> <pre><code>Perry, Katy (Cartographer)\n</code></pre>"},{"location":"recipes/standardize-creators/#corporate-body-names","title":"Corporate Body Names","text":""},{"location":"recipes/standardize-creators/#abbreviations-and-initialisms","title":"Abbreviations and Initialisms","text":"<p>Regardless of how a name appears on the resource, always use the spelled out form of the name as opposed to abbreviations or initialisms for the purpose of being clear. For example, use </p> <p> United States Geological Survey</p> <p> U.S.G.S. </p> <p> USGS</p> <p> Cook County Geographic Information Systems</p> <p> Cook County GIS</p>"},{"location":"recipes/standardize-creators/#subordinate-bodies","title":"Subordinate Bodies","text":"<p>A \u201csubordinate body\u201d is a corporate entity that is part of another corporate entity. To avoid confusion for Geoportal users, always include the name of the larger \u201cparent\u201d entity. For instance:</p> <p> Cheyenne Light, Fuel and Power Company. Engineering Department</p> <p> Engineering Department</p> <p> Canada. Department of the Interior</p> <p> Department of the Interior</p>"},{"location":"recipes/standardize-creators/#jurisdictional-geographic-names-used-in-the-creator-field","title":"Jurisdictional Geographic Names Used in the Creator Field","text":"<p>For background, \u201cjurisdictional\u201d place names are those that are defined legally by a set of boundaries and overseen by a governmental agency. In the United States these would be cities, towns, townships, boroughs, villages (mostly), counties, states and so forth. Non-jurisdictional places are of two types: either entities in nature that have been given a name such as the Mississippi River or Rocky Mountains, or are administrative component areas of a larger formal jurisdiction such as ranger districts within a national forest.</p> <p>It will be extremely rare NOT to find an authorized form of a jurisdictional place name in FAST and LCNAF. However, if you encounter a place not found in these resources, follow the pattern used in FAST.</p> <p>For a dataset in which the creator name is given as \u201cCity of Kenosha\u201d, FAST formulates the jurisdictional place name as:</p> <pre><code>    Wisconsin--Kenosha\n</code></pre>"},{"location":"recipes/standardize-creators/#directions-for-standardizing-metadata-records-for-the-btaa-geoportal","title":"Directions for standardizing metadata records for the BTAA Geoportal","text":"<p>When a name is not found in FAST, search the Library of Congress Name Authority File (LCNAF) for a match following the directions in the Searching LCNAF section below. If a matching name is found in LCNAF, a FAST record may be requested, as explained below.</p> <p>If no match is found, there\u2019s no requirement to do intensive research. Instead, follow the direction in the section, Guidance for Formulating Creator Names Not Present in the Registries noted above.</p>"},{"location":"recipes/standardize-creators/#searching-lcnaf_1","title":"Searching LCNAF","text":"<p>To search the LCNAF for a creator name, you may use either the Library of Congress Authorities or the LC Linked Data Service. The LC Linked Data Service is ideal for quick keyword searches, while Library of Congress Authorities allows for browse searching.</p> <p>Searching the LC Linked Data Service:</p> <ul> <li>Go to https://id.loc.gov/authorities/names.html </li> <li>Type the creator name into the text box and press enter or click on Go</li> <li>The result that appears in the \"Label\" column is the LCNAF authorized form of the name, for instance, <code>Cumberland County (Pa.)</code></li> <li>The number at the far right in the \"Identifier\" column is the Library of Congress control number (LCCN), which for Cumberland County Pennsylvania is <code>n81032665</code></li> </ul> <p>Often, when searching for names of persons, several results appear to be possible matches. In those cases, click on a heading in the results list and look for the \"Sources\" heading to see a list of citations that have been associated with that entity.</p> <p>When a name is found in the LCNAF, submit a request that the LCNAF name be added to FAST by following the steps below.</p>"},{"location":"recipes/standardize-creators/#request-additions-to-fast","title":"Request Additions to FAST","text":"<p>You will use the importFAST Subject Headings utility to request LCNAF additions to FAST.</p> <ol> <li>First, select the type of LCNAF name.<ol> <li>Personal Name</li> <li>Corporate Name</li> <li>Topical: We are unlikely to use this one for a creator.</li> </ol> </li> <li>Copy the \u201cIdentifier\u201d (the Library of Congress control number) from the LCNAF record and paste it into the \u201center name LCCN\u201d text box. Click the \u201cImport\u201d button.</li> <li>The form will automatically populate using the LCNAF name. Check to make sure the correct name has been imported.</li> <li>Enter the email address <code>geoportal@btaa.org</code> (you do not need to fill out the \u201cAnything extra\u201d text box) and click \u201cSubmit Heading\u201d.</li> <li>The new FAST heading will appear! Please copy the FAST heading number from the end of the string (e.g. \u201c<code>fst02013467</code>\u201d) and paste it into the spreadsheet to show that the heading has been added to FAST.</li> <li>Enter the newly created FAST heading into the spreadsheet.</li> </ol> <p>Geographic names cannot be requested through the importFAST Subject Headings tool. In the spreadsheet, place \u201cYes\u201d in the column labeled \u201cRequest FAST geographic name\u201d.</p>"},{"location":"recipes/update-hub-list/","title":"How to update the list of ArcGIS Hub websites.","text":""},{"location":"recipes/update-hub-list/#background","title":"Background","text":"<p>The BTAA Geoportal provides a central access point to find and browse public geospatial data. A large portion of these records come from ArcGIS Hubs that are maintained by states, counties, cities, and regional entities. These entities continually update the data and the website platforms. In turn, we need to continually update our links to these resources.</p> <p>The ArcGIS Harvesting Recipe walks through how we programmatically query the ArcGIS Hubs to obtain the current list of datasets.  This page describes how to keep our list of ArcGIS Hub websites updated.</p>"},{"location":"recipes/update-hub-list/#how-to-create-or-troubleshoot-an-arcgis-hub-website-record-in-gbl-admin","title":"How to create or troubleshoot an ArcGIS Hub website record in GBL Admin","text":"<p>Info</p> <p>The highlighted fields listed below are required for the ArcGIS harvesting script. If the script fails, check that these fields have been added. The underlined fields are used to query GBL Admin and produce the list of Hubs that we regularly harvest.</p> <p>Each Hub has its own entry in GBL Admin. Manually create or update each record with the following parameters:</p> <ul> <li>Title: The name of the site as shown on its homepage. This value will be transferred into the Provider field of each dataset.</li> <li>Description: Usually \"Website for finding and accessing open data provided by \" followed by the name of the administrative place or organization publishing the site. Additional descriptions are helpful.</li> <li>Language: 3-letter code as shown on the OpenGeoMetadata website.</li> <li>Publisher: The administrative place or organization publishing the site. This value will be concatenated into the title of each dataset. For place names, use the FAST format (i.e. <code>Minnesota--Clay County</code>.</li> <li>Resource Class: <code>Websites</code> This value is used for filtering &amp; finding the Hubs in GBL Admin</li> <li>Temporal Coverage: <code>Continually updated resource</code></li> <li>Spatial Coverage: Add place names using the FAST format as described for the B1G Profile.</li> <li>Bounding Box: If the Hub covers a specific area, include a bounding box for it using the manual method described in the Add bounding boxes recipe.</li> <li>Member Of: one of the following values: <ul> <li><code>ba5cc745-21c5-4ae9-954b-72dd8db6815a</code>  (Government Open Geospatial Data)</li> <li><code>b0153110-e455-4ced-9114-9b13250a7093</code> (Research Institutes Geospatial Data Collection)</li> </ul> </li> <li>Format: <code>ArcGIS Hub</code> This value is used for filtering &amp; finding the Hubs in GBL Admin</li> <li>Links - Reference - \"Full layer description\" : link to the homepage for the Hub</li> <li>ID and Code: Both of the values will be the same. Create a new code by following the description on the Code Naming Schema page. Use the Advanced Search in GBL Admin to query which codes have already been used. If it is not clear what code to create, ask the Product Manager or use the UUID Generator website to create a random one. The ID value will be transferred into the Code field of each dataset.</li> <li> <p>Identifier: If the record will be part of the monthly harvests, add this to the end of the baseUrl (usually the homepage): <code>/api/feed/dcat-us/1.1.json</code>. The Identifier will be used to query the metadata for the website.</p> <p>Warning</p> <p>Always check the Identifier link! It should show a JSON API in your browser that displays all of the metadata for each dataset hosted by the website. The baseUrl may be slightly different than the landing page for the organization. For example, some entities may add the string \"data-\" to the beginning of their site URL. The best way to make sure you have the right URL is to look for a box labeled \"Search all data\". This will result in a link like \"<code>baseUrl</code>/search\". Then, replace the \"/search\" with <code>/api/feed/dcat-us/1.1.json</code></p> </li> <li> <p>Access Rights: <code>Public</code> for all ArcGIS Hubs.</p> </li> <li>Accrual Method: <code>DCAT US 1.1</code>. This value is used for filtering &amp; finding the Hubs in GBL Admin</li> <li>Status: If the site is part of the ArcGIS Harvest, use the value <code>Indexed</code>. If the site is not part of the harvest, use <code>Not indexed</code>. Other explanatory text can be included here, such as indicating if the site is broken.</li> <li>Publication State: When a new record is created it will automatically be assigned <code>Draft</code>. Change the state to <code>published</code> when the metadata record is ready. If the site breaks or is deprecated, change this value to <code>unpublished</code>.</li> </ul>"},{"location":"recipes/update-hub-list/#how-to-remove-broken-or-deprecated-arcgis-hubs","title":"How to remove broken or deprecated ArcGIS Hubs","text":"<p>If a Hub site stalls the Harvesting script, it needs to be updated in GBL Admin.</p>"},{"location":"recipes/update-hub-list/#if-the-site-is-missing","title":"If the site is missing:","text":"<p>Try to find a replacement site. When a Hub is updated to a new version, sometimes the baseURL will change. If a new site is found, update:</p> <ul> <li>Links - Reference - \"Full layer description\" : new landing page </li> <li>Identifier : the new API (with the suffix <code>/api/feed/dcat-us/1.1.json</code>)</li> </ul>"},{"location":"recipes/update-hub-list/#if-the-site-is-present-but-the-api-is-returning-an-error","title":"If the site is present, but the API is returning an error:","text":"<p>In this case, we freeze the website and the dataset records, but stop new harvests. Make these changes:</p>"},{"location":"recipes/update-hub-list/#website-record","title":"Website Record","text":"<ul> <li>Accrual Method: remove <code>DCAT US 1.1</code> and leave blank</li> <li>Status: Change the value \"Indexed\" to \"Not indexed\". Leave a short explanation if the API is broken.</li> </ul>"},{"location":"recipes/update-hub-list/#dataset-records","title":"Dataset Records","text":"<ul> <li>Export all of the records using the <code>Code</code> field</li> <li>Accrual Method: change from \"ArcGIS Hub\" to \"ArcGISHub-paused\"</li> </ul> <p>Broken APIs</p> <p>These steps will remove the records from our preset query to select active ArcGIS Hubs in GBL Admin. The effect will be to freeze them until the API starts working again. Once the API becomes accessible again, reverse the Accrual Method values.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/orientation/","title":"Technology Orientation","text":"<p>These slides accompany an orientation for Team Members to learn more about the GeoBTAA metadata profile, finding content for the BTAA Geoportal, tricky fields, and metadata workflows.</p> <p>View the slides</p>"},{"location":"tutorials/python-jupyter/","title":"Harvesting","text":"<p>These tutorials are short, easy to complete exercises to help someone get the basics of running and writing scripts to harvest metadata. They are available as Jupyter Notebooks hosted in GitHub in the Harvesting Guide repository.</p>"},{"location":"tutorials/python-jupyter/#1-setting-up-your-environment","title":"1. Setting up your environment","text":"<ul> <li>These tutorials will guide users on how to set up your environment for harvesting.</li> <li>Getting Started with GitHub: Provides an introduction to GitHub and a walkthrough of creating a repository.</li> <li>Getting Started with Jupyter Notebooks: Provides an introduction and overview of cell types.</li> </ul>"},{"location":"tutorials/python-jupyter/#2-navigating-paths","title":"2. Navigating paths","text":"<p>This tutorial shows how to navigate and list directories. Techniques covered include:</p> <ul> <li>Using the <code>cd</code> command in the terminal</li> <li>Navigating to and from the home directory</li> <li>Listing the current path</li> <li>Listing documents within the current path</li> </ul>"},{"location":"tutorials/python-jupyter/#3-iterating-over-files","title":"3. Iterating over files","text":"<ul> <li>This guide will assist users in how to open, read, and print the results of a CSV.</li> <li>The module <code>os.Walk</code> will be introduced to read through multiple directories to find files.</li> <li>The pandas module will be used to display a CSV for records.</li> </ul>"},{"location":"tutorials/python-jupyter/#4-merge-csv-files-based-on-a-shared-column","title":"4. Merge CSV files based on a shared column","text":"<p>This tutorial will take two CSV files and combine them using a shared key.</p>"},{"location":"tutorials/python-jupyter/#5-transform-a-batch-of-json-files-into-a-single-csv-file","title":"5. Transform a batch of JSON files into a single CSV file","text":"<p>This tutorial uses the Python module pandas (Python Data Analysis Library) to open a batch of JSON files and transform the contents into a single CSV.</p>"},{"location":"tutorials/python-jupyter/#6-extract-place-names","title":"6. Extract Place Names","text":"<p>This tutorial scans the two columns from a CSV file ('Title' and 'Description') to look for known place names and writes the values to a separate field.</p>"},{"location":"tutorials/python-jupyter/#7-parsing-html-with-beautiful-soup","title":"7. Parsing HTML with Beautiful Soup","text":"<p>This tutorial will guide users through Hyper Text Mark-Up Language (HTML) site parsing using the BeautifulSoup Python module, including:</p> <ul> <li>how to install the BeautifulSoup module</li> <li>scan and list web pages</li> <li>return titles, descriptions, and dates</li> <li>writing parsed results to CSV format</li> </ul>"},{"location":"tutorials/python-jupyter/#8-use-openstreetmap-to-generate-bounding-boxes","title":"8. Use OpenStreetMap to generate bounding boxes","text":"<p>This tutorial demonstrates how to query the OpenStreetMap API using place names to return bounding box coordinates.</p> <p>Credits</p> <p>These tutorials were prepared by Alexander Danielson and Karen Majewicz in April 2023.</p>"}]}